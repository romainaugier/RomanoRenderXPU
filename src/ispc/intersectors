	.text
	.def	 @feat.00;
	.scl	3;
	.type	0;
	.endef
	.globl	@feat.00
.set @feat.00, 0
	.file	"intersectors.ispc"
	.def	 "operator-___s_5B_vyvec3_5D_s_5B_vyvec3_5D_";
	.scl	2;
	.type	32;
	.endef
	.globl	"operator-___s_5B_vyvec3_5D_s_5B_vyvec3_5D_" # -- Begin function operator-___s_5B_vyvec3_5D_s_5B_vyvec3_5D_
	.p2align	4, 0x90
"operator-___s_5B_vyvec3_5D_s_5B_vyvec3_5D_": # @operator-___s_5B_vyvec3_5D_s_5B_vyvec3_5D_
# %bb.0:                                # %allocas
	vmovaps	(%r8), %ymm2
	vmovaps	(%rdx), %ymm1
	vmovaps	(%rcx), %ymm0
	movq	48(%rsp), %rax
	movq	40(%rsp), %rcx
	vsubps	(%r9), %ymm0, %ymm0
	vsubps	(%rcx), %ymm1, %ymm1
	vsubps	(%rax), %ymm2, %ymm2
	retq
                                        # -- End function
	.def	 SphereHitN___un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_unf_3E_uni;
	.scl	2;
	.type	32;
	.endef
	.globl	__real@3f800000                 # -- Begin function SphereHitN___un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_unf_3E_uni
	.section	.rdata,"dr",discard,__real@3f800000
	.p2align	2
__real@3f800000:
	.long	0x3f800000                      # float 1
	.globl	__real@ffffff82
	.section	.rdata,"dr",discard,__real@ffffff82
	.p2align	2
__real@ffffff82:
	.long	4294967170                      # 0xffffff82
	.globl	__real@807fffff
	.section	.rdata,"dr",discard,__real@807fffff
	.p2align	2
__real@807fffff:
	.long	2155872255                      # 0x807fffff
	.globl	__real@3f000000
	.section	.rdata,"dr",discard,__real@3f000000
	.p2align	2
__real@3f000000:
	.long	1056964608                      # 0x3f000000
	.globl	__real@3fdfe28e
	.section	.rdata,"dr",discard,__real@3fdfe28e
	.p2align	2
__real@3fdfe28e:
	.long	0x3fdfe28e                      # float 1.7491014
	.globl	__real@c01f5af7
	.section	.rdata,"dr",discard,__real@c01f5af7
	.p2align	2
__real@c01f5af7:
	.long	0xc01f5af7                      # float -2.48992705
	.globl	__real@3ffe0193
	.section	.rdata,"dr",discard,__real@3ffe0193
	.p2align	2
__real@3ffe0193:
	.long	0x3ffe0193                      # float 1.98442304
	.globl	__real@bf198181
	.section	.rdata,"dr",discard,__real@bf198181
	.p2align	2
__real@bf198181:
	.long	0xbf198181                      # float -0.599632323
	.globl	__real@3eaf548d
	.section	.rdata,"dr",discard,__real@3eaf548d
	.p2align	2
__real@3eaf548d:
	.long	0x3eaf548d                      # float 0.342441946
	.globl	__real@3e33a0af
	.section	.rdata,"dr",discard,__real@3e33a0af
	.p2align	2
__real@3e33a0af:
	.long	0x3e33a0af                      # float 0.175417647
	.globl	__real@3e80fb87
	.section	.rdata,"dr",discard,__real@3e80fb87
	.p2align	2
__real@3e80fb87:
	.long	0x3e80fb87                      # float 0.251919001
	.globl	__real@3eaaa11c
	.section	.rdata,"dr",discard,__real@3eaaa11c
	.p2align	2
__real@3eaaa11c:
	.long	0x3eaaa11c                      # float 0.333260417
	.globl	__real@3f000010
	.section	.rdata,"dr",discard,__real@3f000010
	.p2align	2
__real@3f000010:
	.long	0x3f000010                      # float 0.500000954
	.globl	__real@3f317218
	.section	.rdata,"dr",discard,__real@3f317218
	.p2align	2
__real@3f317218:
	.long	0x3f317218                      # float 0.693147182
	.globl	__real@7fc00000
	.section	.rdata,"dr",discard,__real@7fc00000
	.p2align	2
__real@7fc00000:
	.long	0x7fc00000                      # float NaN
	.globl	__real@ff800000
	.section	.rdata,"dr",discard,__real@ff800000
	.p2align	2
__real@ff800000:
	.long	0xff800000                      # float -Inf
	.globl	__real@3fb8aa3b
	.section	.rdata,"dr",discard,__real@3fb8aa3b
	.p2align	2
__real@3fb8aa3b:
	.long	0x3fb8aa3b                      # float 1.44269502
	.globl	__real@3f317200
	.section	.rdata,"dr",discard,__real@3f317200
	.p2align	2
__real@3f317200:
	.long	0x3f317200                      # float 0.693145751
	.globl	__real@35bfbe8e
	.section	.rdata,"dr",discard,__real@35bfbe8e
	.p2align	2
__real@35bfbe8e:
	.long	0x35bfbe8e                      # float 1.42860677E-6
	.globl	__real@39907835
	.section	.rdata,"dr",discard,__real@39907835
	.p2align	2
__real@39907835:
	.long	0x39907835                      # float 2.75553815E-4
	.globl	__real@3aaaf7b5
	.section	.rdata,"dr",discard,__real@3aaaf7b5
	.p2align	2
__real@3aaaf7b5:
	.long	0x3aaaf7b5                      # float 0.00130437932
	.globl	__real@3c09475d
	.section	.rdata,"dr",discard,__real@3c09475d
	.p2align	2
__real@3c09475d:
	.long	0x3c09475d                      # float 0.00837883074
	.globl	__real@3d2a9d49
	.section	.rdata,"dr",discard,__real@3d2a9d49
	.p2align	2
__real@3d2a9d49:
	.long	0x3d2a9d49                      # float 0.0416539051
	.globl	__real@3e2aab20
	.section	.rdata,"dr",discard,__real@3e2aab20
	.p2align	2
__real@3e2aab20:
	.long	0x3e2aab20                      # float 0.166668415
	.globl	__real@3efffffd
	.section	.rdata,"dr",discard,__real@3efffffd
	.p2align	2
__real@3efffffd:
	.long	0x3efffffd                      # float 0.499999911
	.globl	__real@0000007f
	.section	.rdata,"dr",discard,__real@0000007f
	.p2align	2
__real@0000007f:
	.long	127                             # 0x7f
	.globl	__real@00000001
	.section	.rdata,"dr",discard,__real@00000001
	.p2align	2
__real@00000001:
	.long	1                               # 0x1
	.globl	__real@7f800000
	.section	.rdata,"dr",discard,__real@7f800000
	.p2align	2
__real@7f800000:
	.long	0x7f800000                      # float +Inf
	.globl	__real@80000000
	.section	.rdata,"dr",discard,__real@80000000
	.p2align	2
__real@80000000:
	.long	0x80000000                      # float -0
	.globl	__real@4b189680
	.section	.rdata,"dr",discard,__real@4b189680
	.p2align	2
__real@4b189680:
	.long	0x4b189680                      # float 1.0E+7
	.globl	__ymm@0000000700000006000000050000000400000003000000020000000100000000
	.section	.rdata,"dr",discard,__ymm@0000000700000006000000050000000400000003000000020000000100000000
	.p2align	5
__ymm@0000000700000006000000050000000400000003000000020000000100000000:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
	.globl	__xmm@ffffffffffffffffffffffffffffffff
	.section	.rdata,"dr",discard,__xmm@ffffffffffffffffffffffffffffffff
	.p2align	4
__xmm@ffffffffffffffffffffffffffffffff:
	.zero	16,255
	.globl	__ymm@0000000000000000000000000000000000000000000000000000000000000000
	.section	.rdata,"dr",discard,__ymm@0000000000000000000000000000000000000000000000000000000000000000
	.p2align	5
__ymm@0000000000000000000000000000000000000000000000000000000000000000:
	.zero	32
	.text
	.globl	SphereHitN___un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_unf_3E_uni
	.p2align	4, 0x90
SphereHitN___un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_unf_3E_uni: # @SphereHitN___un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_Cunf_3E_un_3C_unf_3E_uni
# %bb.0:                                # %allocas
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rsi
	pushq	%rdi
	pushq	%rbp
	pushq	%rbx
	subq	$1208, %rsp                     # imm = 0x4B8
	vmovaps	%xmm15, 1184(%rsp)              # 16-byte Spill
	vmovaps	%xmm14, 1168(%rsp)              # 16-byte Spill
	vmovaps	%xmm13, 1152(%rsp)              # 16-byte Spill
	vmovdqa	%xmm12, 1136(%rsp)              # 16-byte Spill
	vmovaps	%xmm11, 1120(%rsp)              # 16-byte Spill
	vmovaps	%xmm10, 1104(%rsp)              # 16-byte Spill
	vmovaps	%xmm9, 1088(%rsp)               # 16-byte Spill
	vmovaps	%xmm8, 1072(%rsp)               # 16-byte Spill
	vmovaps	%xmm7, 1056(%rsp)               # 16-byte Spill
	vmovaps	%xmm6, 1040(%rsp)               # 16-byte Spill
	movl	1368(%rsp), %eax
	movq	1360(%rsp), %r12
	movq	1352(%rsp), %r13
	movq	1344(%rsp), %r11
	movq	1336(%rsp), %rsi
	movq	1312(%rsp), %r14
	leal	7(%rax), %ebp
	testl	%eax, %eax
	cmovnsl	%eax, %ebp
	andl	$-8, %ebp
	testl	%ebp, %ebp
	jle	.LBB1_1
# %bb.4:                                # %foreach_full_body.lr.ph
	xorl	%eax, %eax
	vbroadcastss	__real@3f800000(%rip), %ymm1 # ymm1 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	__real@ffffff82(%rip), %ymm2 # ymm2 = [4294967170,4294967170,4294967170,4294967170,4294967170,4294967170,4294967170,4294967170]
	vmovups	%ymm2, 736(%rsp)                # 32-byte Spill
	vbroadcastss	__real@807fffff(%rip), %ymm2 # ymm2 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vmovups	%ymm2, 704(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3f000000(%rip), %ymm2 # ymm2 = [1056964608,1056964608,1056964608,1056964608,1056964608,1056964608,1056964608,1056964608]
	vmovups	%ymm2, 256(%rsp)                # 32-byte Spill
	vpbroadcastd	__real@3fdfe28e(%rip), %ymm2 # ymm2 = [1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0]
	vmovdqu	%ymm2, 224(%rsp)                # 32-byte Spill
	vbroadcastss	__real@c01f5af7(%rip), %ymm0 # ymm0 = [-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0]
	vmovups	%ymm0, (%rsp)                   # 32-byte Spill
	vbroadcastss	__real@3ffe0193(%rip), %ymm0 # ymm0 = [1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0]
	vmovups	%ymm0, 32(%rsp)                 # 32-byte Spill
	vbroadcastss	__real@bf198181(%rip), %ymm0 # ymm0 = [-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1]
	vmovups	%ymm0, 64(%rsp)                 # 32-byte Spill
	vbroadcastss	__real@3eaf548d(%rip), %ymm0 # ymm0 = [3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1]
	vmovups	%ymm0, 768(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3e33a0af(%rip), %ymm0 # ymm0 = [1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1]
	vmovups	%ymm0, 672(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3e80fb87(%rip), %ymm0 # ymm0 = [2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1]
	vmovups	%ymm0, 640(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3eaaa11c(%rip), %ymm0 # ymm0 = [3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1]
	vmovups	%ymm0, 608(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3f000010(%rip), %ymm0 # ymm0 = [5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1]
	vmovups	%ymm0, 192(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3f317218(%rip), %ymm0 # ymm0 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	vmovups	%ymm0, 576(%rsp)                # 32-byte Spill
	vbroadcastss	__real@7fc00000(%rip), %ymm0 # ymm0 = [NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN]
	vmovups	%ymm0, 544(%rsp)                # 32-byte Spill
	xorl	%edi, %edi
	vbroadcastss	__real@ff800000(%rip), %ymm0 # ymm0 = [-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf]
	vmovups	%ymm0, 512(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3fb8aa3b(%rip), %ymm0 # ymm0 = [1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0]
	vmovups	%ymm0, 160(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3f317200(%rip), %ymm0 # ymm0 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vmovups	%ymm0, 128(%rsp)                # 32-byte Spill
	vbroadcastss	__real@35bfbe8e(%rip), %ymm0 # ymm0 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vmovups	%ymm0, 480(%rsp)                # 32-byte Spill
	vbroadcastss	__real@39907835(%rip), %ymm0 # ymm0 = [2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4]
	vmovups	%ymm0, 928(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3aaaf7b5(%rip), %ymm0 # ymm0 = [1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3]
	vmovups	%ymm0, 96(%rsp)                 # 32-byte Spill
	vbroadcastss	__real@3c09475d(%rip), %ymm0 # ymm0 = [8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3]
	vmovups	%ymm0, 320(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3d2a9d49(%rip), %ymm0 # ymm0 = [4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2]
	vmovups	%ymm0, 288(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3e2aab20(%rip), %ymm0 # ymm0 = [1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1]
	vmovups	%ymm0, 448(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3efffffd(%rip), %ymm0 # ymm0 = [4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1]
	vmovups	%ymm0, 416(%rsp)                # 32-byte Spill
	vbroadcastss	__real@0000007f(%rip), %ymm0 # ymm0 = [127,127,127,127,127,127,127,127]
	vmovups	%ymm0, 352(%rsp)                # 32-byte Spill
	vbroadcastss	__real@00000001(%rip), %ymm0 # ymm0 = [1,1,1,1,1,1,1,1]
	vmovups	%ymm0, 896(%rsp)                # 32-byte Spill
	vbroadcastss	__real@7f800000(%rip), %ymm0 # ymm0 = [+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf]
	vmovups	%ymm0, 384(%rsp)                # 32-byte Spill
	jmp	.LBB1_5
	.p2align	4, 0x90
.LBB1_7:                                # %if_done
                                        #   in Loop: Header=BB1_5 Depth=1
	addl	$8, %edi
	addl	$32, %eax
	cmpl	%ebp, %edi
	jge	.LBB1_2
.LBB1_5:                                # %foreach_full_body
                                        # =>This Inner Loop Header: Depth=1
	movslq	%eax, %r15
	vmovups	(%rsi,%r15), %ymm4
	vmovups	%ymm4, 864(%rsp)                # 32-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vcmpltps	%ymm0, %ymm4, %ymm2
	vcmpleps	%ymm0, %ymm4, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm4, %ymm4
	vpsrad	$23, %ymm4, %ymm6
	vmovdqu	736(%rsp), %ymm0                # 32-byte Reload
	vpaddd	%ymm0, %ymm6, %ymm6
	vmovdqa	%ymm0, %ymm10
	vmovups	704(%rsp), %ymm13               # 32-byte Reload
	vandps	%ymm4, %ymm13, %ymm4
	vorps	256(%rsp), %ymm4, %ymm4         # 32-byte Folded Reload
	vsubps	%ymm4, %ymm1, %ymm4
	vmovups	224(%rsp), %ymm11               # 32-byte Reload
	vmovaps	%ymm11, %ymm8
	vmovups	(%rsp), %ymm0                   # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	32(%rsp), %ymm0                 # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	64(%rsp), %ymm0                 # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	768(%rsp), %ymm7                # 32-byte Reload
	vfmadd213ps	%ymm7, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm7
	vmovups	672(%rsp), %ymm0                # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vfmadd213ps	640(%rsp), %ymm4, %ymm8 # 32-byte Folded Reload
                                        # ymm8 = (ymm4 * ymm8) + mem
	vmovups	608(%rsp), %ymm5                # 32-byte Reload
	vfmadd213ps	%ymm5, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm5
	vmovups	192(%rsp), %ymm9                # 32-byte Reload
	vfmadd213ps	%ymm9, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm9
	vfmadd213ps	%ymm1, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm1
	vcvtdq2ps	%ymm6, %ymm6
	vmulps	%ymm4, %ymm8, %ymm4
	vfmsub231ps	576(%rsp), %ymm6, %ymm4 # 32-byte Folded Reload
                                        # ymm4 = (ymm6 * mem) - ymm4
	vmovups	512(%rsp), %ymm6                # 32-byte Reload
	vblendvps	%ymm2, 544(%rsp), %ymm6, %ymm2 # 32-byte Folded Reload
	vblendvps	%ymm3, %ymm2, %ymm4, %ymm2
	vaddps	%ymm2, %ymm2, %ymm2
	vmovups	160(%rsp), %ymm14               # 32-byte Reload
	vmulps	%ymm2, %ymm14, %ymm3
	vroundps	$9, %ymm3, %ymm3
	vcvttps2dq	%ymm3, %ymm4
	vfmsub231ps	128(%rsp), %ymm3, %ymm2 # 32-byte Folded Reload
                                        # ymm2 = (ymm3 * mem) - ymm2
	vfnmsub231ps	480(%rsp), %ymm3, %ymm2 # 32-byte Folded Reload
                                        # ymm2 = -(ymm3 * mem) - ymm2
	vmovups	928(%rsp), %ymm15               # 32-byte Reload
	vmovaps	%ymm15, %ymm3
	vmovups	96(%rsp), %ymm12                # 32-byte Reload
	vfmadd213ps	%ymm12, %ymm2, %ymm3    # ymm3 = (ymm2 * ymm3) + ymm12
	vfmadd213ps	320(%rsp), %ymm2, %ymm3 # 32-byte Folded Reload
                                        # ymm3 = (ymm2 * ymm3) + mem
	vmovups	288(%rsp), %ymm6                # 32-byte Reload
	vfmadd213ps	%ymm6, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm3) + ymm6
	vmovups	448(%rsp), %ymm6                # 32-byte Reload
	vfmadd213ps	%ymm6, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm3) + ymm6
	vmovups	416(%rsp), %ymm6                # 32-byte Reload
	vfmadd213ps	%ymm6, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm3) + ymm6
	vfmadd213ps	%ymm1, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm3) + ymm1
	vfmadd213ps	%ymm1, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm3) + ymm1
	vmovdqu	352(%rsp), %ymm6                # 32-byte Reload
	vpaddd	%ymm6, %ymm4, %ymm2
	vpcmpgtd	%ymm6, %ymm4, %ymm4
	vpslld	$23, %ymm2, %ymm6
	vmulps	%ymm6, %ymm3, %ymm3
	vblendvps	%ymm4, 384(%rsp), %ymm3, %ymm3 # 32-byte Folded Reload
	vmovdqu	896(%rsp), %ymm4                # 32-byte Reload
	vpcmpgtd	%ymm2, %ymm4, %ymm2
	vpandn	%ymm3, %ymm2, %ymm2
	vmovdqu	%ymm2, 832(%rsp)                # 32-byte Spill
	vmovups	(%r11,%r15), %ymm2
	vmovups	%ymm2, 800(%rsp)                # 32-byte Spill
	vxorps	%xmm3, %xmm3, %xmm3
	vcmpltps	%ymm3, %ymm2, %ymm4
	vcmpleps	%ymm3, %ymm2, %ymm6
	vblendvps	%ymm6, %ymm1, %ymm2, %ymm8
	vpsrad	$23, %ymm8, %ymm9
	vpaddd	%ymm10, %ymm9, %ymm9
	vmovaps	%ymm13, %ymm2
	vandps	%ymm13, %ymm8, %ymm8
	vmovups	256(%rsp), %ymm13               # 32-byte Reload
	vorps	%ymm13, %ymm8, %ymm8
	vsubps	%ymm8, %ymm1, %ymm8
	vmovaps	%ymm11, %ymm10
	vfmadd213ps	(%rsp), %ymm8, %ymm10   # 32-byte Folded Reload
                                        # ymm10 = (ymm8 * ymm10) + mem
	vfmadd213ps	32(%rsp), %ymm8, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm8 * ymm10) + mem
	vfmadd213ps	64(%rsp), %ymm8, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm8 * ymm10) + mem
	vfmadd213ps	%ymm7, %ymm8, %ymm10    # ymm10 = (ymm8 * ymm10) + ymm7
	vfmadd213ps	%ymm0, %ymm8, %ymm10    # ymm10 = (ymm8 * ymm10) + ymm0
	vmovups	640(%rsp), %ymm11               # 32-byte Reload
	vfmadd213ps	%ymm11, %ymm8, %ymm10   # ymm10 = (ymm8 * ymm10) + ymm11
	vfmadd213ps	%ymm5, %ymm8, %ymm10    # ymm10 = (ymm8 * ymm10) + ymm5
	vfmadd213ps	192(%rsp), %ymm8, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm8 * ymm10) + mem
	vfmadd213ps	%ymm1, %ymm8, %ymm10    # ymm10 = (ymm8 * ymm10) + ymm1
	vcvtdq2ps	%ymm9, %ymm9
	vmulps	%ymm10, %ymm8, %ymm8
	vmovups	544(%rsp), %ymm0                # 32-byte Reload
	vmovups	512(%rsp), %ymm7                # 32-byte Reload
	vblendvps	%ymm4, %ymm0, %ymm7, %ymm4
	vfmsub231ps	576(%rsp), %ymm9, %ymm8 # 32-byte Folded Reload
                                        # ymm8 = (ymm9 * mem) - ymm8
	vblendvps	%ymm6, %ymm4, %ymm8, %ymm4
	vaddps	%ymm4, %ymm4, %ymm4
	vmulps	%ymm4, %ymm14, %ymm6
	vroundps	$9, %ymm6, %ymm6
	vcvttps2dq	%ymm6, %ymm8
	vfmsub231ps	128(%rsp), %ymm6, %ymm4 # 32-byte Folded Reload
                                        # ymm4 = (ymm6 * mem) - ymm4
	vfnmsub231ps	480(%rsp), %ymm6, %ymm4 # 32-byte Folded Reload
                                        # ymm4 = -(ymm6 * mem) - ymm4
	vfmadd213ps	%ymm12, %ymm4, %ymm15   # ymm15 = (ymm4 * ymm15) + ymm12
	vfmadd213ps	320(%rsp), %ymm4, %ymm15 # 32-byte Folded Reload
                                        # ymm15 = (ymm4 * ymm15) + mem
	vfmadd213ps	288(%rsp), %ymm4, %ymm15 # 32-byte Folded Reload
                                        # ymm15 = (ymm4 * ymm15) + mem
	vfmadd213ps	448(%rsp), %ymm4, %ymm15 # 32-byte Folded Reload
                                        # ymm15 = (ymm4 * ymm15) + mem
	vfmadd213ps	416(%rsp), %ymm4, %ymm15 # 32-byte Folded Reload
                                        # ymm15 = (ymm4 * ymm15) + mem
	vfmadd213ps	%ymm1, %ymm4, %ymm15    # ymm15 = (ymm4 * ymm15) + ymm1
	vfmadd213ps	%ymm1, %ymm4, %ymm15    # ymm15 = (ymm4 * ymm15) + ymm1
	vmovdqu	352(%rsp), %ymm3                # 32-byte Reload
	vpaddd	%ymm3, %ymm8, %ymm4
	vpcmpgtd	%ymm3, %ymm8, %ymm8
	vpslld	$23, %ymm4, %ymm9
	vmulps	%ymm9, %ymm15, %ymm6
	vmovups	384(%rsp), %ymm15               # 32-byte Reload
	vblendvps	%ymm8, %ymm15, %ymm6, %ymm6
	vmovdqu	896(%rsp), %ymm3                # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm3, %ymm4
	vpandn	%ymm6, %ymm4, %ymm4
	vaddps	832(%rsp), %ymm4, %ymm4         # 32-byte Folded Reload
	vmovups	(%r13,%r15), %ymm9
	vmovups	%ymm9, 992(%rsp)                # 32-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vcmpltps	%ymm5, %ymm9, %ymm6
	vcmpleps	%ymm5, %ymm9, %ymm8
	vblendvps	%ymm8, %ymm1, %ymm9, %ymm9
	vpsrad	$23, %ymm9, %ymm10
	vpaddd	736(%rsp), %ymm10, %ymm10       # 32-byte Folded Reload
	vandps	%ymm2, %ymm9, %ymm9
	vorps	%ymm13, %ymm9, %ymm9
	vsubps	%ymm9, %ymm1, %ymm9
	vmovups	224(%rsp), %ymm5                # 32-byte Reload
	vfmadd213ps	(%rsp), %ymm9, %ymm5    # 32-byte Folded Reload
                                        # ymm5 = (ymm9 * ymm5) + mem
	vfmadd213ps	32(%rsp), %ymm9, %ymm5  # 32-byte Folded Reload
                                        # ymm5 = (ymm9 * ymm5) + mem
	vfmadd213ps	64(%rsp), %ymm9, %ymm5  # 32-byte Folded Reload
                                        # ymm5 = (ymm9 * ymm5) + mem
	vmovups	768(%rsp), %ymm14               # 32-byte Reload
	vfmadd213ps	%ymm14, %ymm9, %ymm5    # ymm5 = (ymm9 * ymm5) + ymm14
	vmovups	672(%rsp), %ymm12               # 32-byte Reload
	vfmadd213ps	%ymm12, %ymm9, %ymm5    # ymm5 = (ymm9 * ymm5) + ymm12
	vfmadd213ps	%ymm11, %ymm9, %ymm5    # ymm5 = (ymm9 * ymm5) + ymm11
	vmovups	608(%rsp), %ymm2                # 32-byte Reload
	vfmadd213ps	%ymm2, %ymm9, %ymm5     # ymm5 = (ymm9 * ymm5) + ymm2
	vfmadd213ps	192(%rsp), %ymm9, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm9 * ymm5) + mem
	vfmadd213ps	%ymm1, %ymm9, %ymm5     # ymm5 = (ymm9 * ymm5) + ymm1
	vcvtdq2ps	%ymm10, %ymm10
	vmulps	%ymm5, %ymm9, %ymm5
	vblendvps	%ymm6, %ymm0, %ymm7, %ymm6
	vmovups	576(%rsp), %ymm0                # 32-byte Reload
	vfmsub231ps	%ymm10, %ymm0, %ymm5    # ymm5 = (ymm0 * ymm10) - ymm5
	vblendvps	%ymm8, %ymm6, %ymm5, %ymm5
	vaddps	%ymm5, %ymm5, %ymm5
	vmulps	160(%rsp), %ymm5, %ymm6         # 32-byte Folded Reload
	vroundps	$9, %ymm6, %ymm6
	vcvttps2dq	%ymm6, %ymm8
	vmovups	128(%rsp), %ymm7                # 32-byte Reload
	vfmsub231ps	%ymm7, %ymm6, %ymm5     # ymm5 = (ymm6 * ymm7) - ymm5
	vmovups	480(%rsp), %ymm11               # 32-byte Reload
	vfnmsub231ps	%ymm6, %ymm11, %ymm5    # ymm5 = -(ymm11 * ymm6) - ymm5
	vmovups	928(%rsp), %ymm13               # 32-byte Reload
	vmovaps	%ymm13, %ymm6
	vfmadd213ps	96(%rsp), %ymm5, %ymm6  # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vmovups	320(%rsp), %ymm10               # 32-byte Reload
	vfmadd213ps	%ymm10, %ymm5, %ymm6    # ymm6 = (ymm5 * ymm6) + ymm10
	vfmadd213ps	288(%rsp), %ymm5, %ymm6 # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vfmadd213ps	448(%rsp), %ymm5, %ymm6 # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vfmadd213ps	416(%rsp), %ymm5, %ymm6 # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vfmadd213ps	%ymm1, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm1
	vfmadd213ps	%ymm1, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm1
	vmovdqu	352(%rsp), %ymm9                # 32-byte Reload
	vpaddd	%ymm9, %ymm8, %ymm5
	vpcmpgtd	%ymm9, %ymm8, %ymm8
	vpslld	$23, %ymm5, %ymm9
	vmulps	%ymm6, %ymm9, %ymm6
	vblendvps	%ymm8, %ymm15, %ymm6, %ymm6
	vpcmpgtd	%ymm5, %ymm3, %ymm5
	vpandn	%ymm6, %ymm5, %ymm5
	vaddps	%ymm5, %ymm4, %ymm3
	vmovups	%ymm3, 832(%rsp)                # 32-byte Spill
	vmovups	(%r14,%r15), %ymm4
	vsubps	(%rcx,%r15), %ymm4, %ymm8
	vxorps	%xmm3, %xmm3, %xmm3
	vcmpleps	%ymm3, %ymm8, %ymm4
	vblendvps	%ymm4, %ymm1, %ymm8, %ymm5
	vandps	704(%rsp), %ymm5, %ymm6         # 32-byte Folded Reload
	vorps	256(%rsp), %ymm6, %ymm6         # 32-byte Folded Reload
	vsubps	%ymm6, %ymm1, %ymm6
	vmovups	224(%rsp), %ymm9                # 32-byte Reload
	vfmadd213ps	(%rsp), %ymm6, %ymm9    # 32-byte Folded Reload
                                        # ymm9 = (ymm6 * ymm9) + mem
	vfmadd213ps	32(%rsp), %ymm6, %ymm9  # 32-byte Folded Reload
                                        # ymm9 = (ymm6 * ymm9) + mem
	vfmadd213ps	64(%rsp), %ymm6, %ymm9  # 32-byte Folded Reload
                                        # ymm9 = (ymm6 * ymm9) + mem
	vfmadd213ps	%ymm14, %ymm6, %ymm9    # ymm9 = (ymm6 * ymm9) + ymm14
	vfmadd213ps	%ymm12, %ymm6, %ymm9    # ymm9 = (ymm6 * ymm9) + ymm12
	vmovups	640(%rsp), %ymm15               # 32-byte Reload
	vfmadd213ps	%ymm15, %ymm6, %ymm9    # ymm9 = (ymm6 * ymm9) + ymm15
	vfmadd213ps	%ymm2, %ymm6, %ymm9     # ymm9 = (ymm6 * ymm9) + ymm2
	vmovups	192(%rsp), %ymm14               # 32-byte Reload
	vfmadd213ps	%ymm14, %ymm6, %ymm9    # ymm9 = (ymm6 * ymm9) + ymm14
	vfmadd213ps	%ymm1, %ymm6, %ymm9     # ymm9 = (ymm6 * ymm9) + ymm1
	vmulps	%ymm6, %ymm9, %ymm9
	movq	1320(%rsp), %rbx
	vmovups	(%rbx,%r15), %ymm6
	vsubps	(%rdx,%r15), %ymm6, %ymm6
	vpsrad	$23, %ymm5, %ymm5
	vmovdqu	736(%rsp), %ymm2                # 32-byte Reload
	vpaddd	%ymm2, %ymm5, %ymm5
	vcvtdq2ps	%ymm5, %ymm5
	vfmsub231ps	%ymm5, %ymm0, %ymm9     # ymm9 = (ymm0 * ymm5) - ymm9
	vcmpltps	%ymm3, %ymm8, %ymm5
	vmovups	512(%rsp), %ymm0                # 32-byte Reload
	vblendvps	%ymm5, 544(%rsp), %ymm0, %ymm5 # 32-byte Folded Reload
	vblendvps	%ymm4, %ymm5, %ymm9, %ymm5
	movq	1328(%rsp), %rbx
	vmovups	(%rbx,%r15), %ymm4
	vsubps	(%r8,%r15), %ymm4, %ymm12
	vmovups	%ymm12, 960(%rsp)               # 32-byte Spill
	vmulps	800(%rsp), %ymm6, %ymm3         # 32-byte Folded Reload
	vfmadd231ps	864(%rsp), %ymm8, %ymm3 # 32-byte Folded Reload
                                        # ymm3 = (ymm8 * mem) + ymm3
	vaddps	%ymm5, %ymm5, %ymm5
	vmulps	160(%rsp), %ymm5, %ymm8         # 32-byte Folded Reload
	vroundps	$9, %ymm8, %ymm8
	vcvttps2dq	%ymm8, %ymm9
	vfmsub231ps	%ymm7, %ymm8, %ymm5     # ymm5 = (ymm8 * ymm7) - ymm5
	vfnmsub231ps	%ymm8, %ymm11, %ymm5    # ymm5 = -(ymm11 * ymm8) - ymm5
	vmovaps	%ymm13, %ymm8
	vfmadd213ps	96(%rsp), %ymm5, %ymm8  # 32-byte Folded Reload
                                        # ymm8 = (ymm5 * ymm8) + mem
	vfmadd213ps	%ymm10, %ymm5, %ymm8    # ymm8 = (ymm5 * ymm8) + ymm10
	vmovups	288(%rsp), %ymm4                # 32-byte Reload
	vfmadd213ps	%ymm4, %ymm5, %ymm8     # ymm8 = (ymm5 * ymm8) + ymm4
	vmovups	448(%rsp), %ymm11               # 32-byte Reload
	vfmadd213ps	%ymm11, %ymm5, %ymm8    # ymm8 = (ymm5 * ymm8) + ymm11
	vmovups	416(%rsp), %ymm13               # 32-byte Reload
	vfmadd213ps	%ymm13, %ymm5, %ymm8    # ymm8 = (ymm5 * ymm8) + ymm13
	vfmadd213ps	%ymm1, %ymm5, %ymm8     # ymm8 = (ymm5 * ymm8) + ymm1
	vfmadd213ps	%ymm1, %ymm5, %ymm8     # ymm8 = (ymm5 * ymm8) + ymm1
	vmovdqu	352(%rsp), %ymm7                # 32-byte Reload
	vpaddd	%ymm7, %ymm9, %ymm5
	vpcmpgtd	%ymm7, %ymm9, %ymm9
	vpslld	$23, %ymm5, %ymm10
	vmulps	%ymm10, %ymm8, %ymm8
	vmovups	384(%rsp), %ymm7                # 32-byte Reload
	vblendvps	%ymm9, %ymm7, %ymm8, %ymm8
	vfmadd231ps	992(%rsp), %ymm12, %ymm3 # 32-byte Folded Reload
                                        # ymm3 = (ymm12 * mem) + ymm3
	vmovups	%ymm3, 864(%rsp)                # 32-byte Spill
	vmovdqu	896(%rsp), %ymm12               # 32-byte Reload
	vpcmpgtd	%ymm5, %ymm12, %ymm3
	vpandn	%ymm8, %ymm3, %ymm3
	vmovdqu	%ymm3, 800(%rsp)                # 32-byte Spill
	vcmpleps	__ymm@0000000000000000000000000000000000000000000000000000000000000000(%rip), %ymm6, %ymm5
	vblendvps	%ymm5, %ymm1, %ymm6, %ymm8
	vandps	704(%rsp), %ymm8, %ymm9         # 32-byte Folded Reload
	vorps	256(%rsp), %ymm9, %ymm9         # 32-byte Folded Reload
	vsubps	%ymm9, %ymm1, %ymm9
	vmovups	224(%rsp), %ymm10               # 32-byte Reload
	vfmadd213ps	(%rsp), %ymm9, %ymm10   # 32-byte Folded Reload
                                        # ymm10 = (ymm9 * ymm10) + mem
	vfmadd213ps	32(%rsp), %ymm9, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm9 * ymm10) + mem
	vfmadd213ps	64(%rsp), %ymm9, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm9 * ymm10) + mem
	vmovups	768(%rsp), %ymm3                # 32-byte Reload
	vfmadd213ps	%ymm3, %ymm9, %ymm10    # ymm10 = (ymm9 * ymm10) + ymm3
	vfmadd213ps	672(%rsp), %ymm9, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm9 * ymm10) + mem
	vfmadd213ps	%ymm15, %ymm9, %ymm10   # ymm10 = (ymm9 * ymm10) + ymm15
	vfmadd213ps	608(%rsp), %ymm9, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm9 * ymm10) + mem
	vfmadd213ps	%ymm14, %ymm9, %ymm10   # ymm10 = (ymm9 * ymm10) + ymm14
	vfmadd213ps	%ymm1, %ymm9, %ymm10    # ymm10 = (ymm9 * ymm10) + ymm1
	vmulps	%ymm10, %ymm9, %ymm9
	vcmpltps	__ymm@0000000000000000000000000000000000000000000000000000000000000000(%rip), %ymm6, %ymm6
	vpsrad	$23, %ymm8, %ymm8
	vpaddd	%ymm2, %ymm8, %ymm8
	vcvtdq2ps	%ymm8, %ymm8
	vmovups	576(%rsp), %ymm15               # 32-byte Reload
	vfmsub231ps	%ymm8, %ymm15, %ymm9    # ymm9 = (ymm15 * ymm8) - ymm9
	vmovups	544(%rsp), %ymm14               # 32-byte Reload
	vblendvps	%ymm6, %ymm14, %ymm0, %ymm6
	vblendvps	%ymm5, %ymm6, %ymm9, %ymm5
	vaddps	%ymm5, %ymm5, %ymm5
	vmulps	160(%rsp), %ymm5, %ymm6         # 32-byte Folded Reload
	vroundps	$9, %ymm6, %ymm6
	vcvttps2dq	%ymm6, %ymm8
	vfmsub231ps	128(%rsp), %ymm6, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm6 * mem) - ymm5
	vmovups	480(%rsp), %ymm0                # 32-byte Reload
	vfnmsub231ps	%ymm6, %ymm0, %ymm5     # ymm5 = -(ymm0 * ymm6) - ymm5
	vmovups	928(%rsp), %ymm10               # 32-byte Reload
	vmovaps	%ymm10, %ymm6
	vfmadd213ps	96(%rsp), %ymm5, %ymm6  # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vfmadd213ps	320(%rsp), %ymm5, %ymm6 # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vfmadd213ps	%ymm4, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm4
	vfmadd213ps	%ymm11, %ymm5, %ymm6    # ymm6 = (ymm5 * ymm6) + ymm11
	vfmadd213ps	%ymm13, %ymm5, %ymm6    # ymm6 = (ymm5 * ymm6) + ymm13
	vfmadd213ps	%ymm1, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm1
	vfmadd213ps	%ymm1, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm1
	vmovdqu	352(%rsp), %ymm2                # 32-byte Reload
	vpaddd	%ymm2, %ymm8, %ymm5
	vpcmpgtd	%ymm2, %ymm8, %ymm8
	vpslld	$23, %ymm5, %ymm9
	vmulps	%ymm6, %ymm9, %ymm6
	vblendvps	%ymm8, %ymm7, %ymm6, %ymm6
	vpcmpgtd	%ymm5, %ymm12, %ymm5
	vpandn	%ymm6, %ymm5, %ymm5
	vmovups	960(%rsp), %ymm4                # 32-byte Reload
	vcmpleps	__ymm@0000000000000000000000000000000000000000000000000000000000000000(%rip), %ymm4, %ymm6
	vblendvps	%ymm6, %ymm1, %ymm4, %ymm8
	vaddps	800(%rsp), %ymm5, %ymm7         # 32-byte Folded Reload
	vandps	704(%rsp), %ymm8, %ymm5         # 32-byte Folded Reload
	vorps	256(%rsp), %ymm5, %ymm5         # 32-byte Folded Reload
	vsubps	%ymm5, %ymm1, %ymm5
	vmovups	224(%rsp), %ymm9                # 32-byte Reload
	vfmadd213ps	(%rsp), %ymm5, %ymm9    # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	32(%rsp), %ymm5, %ymm9  # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	64(%rsp), %ymm5, %ymm9  # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	%ymm3, %ymm5, %ymm9     # ymm9 = (ymm5 * ymm9) + ymm3
	vfmadd213ps	672(%rsp), %ymm5, %ymm9 # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	640(%rsp), %ymm5, %ymm9 # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	608(%rsp), %ymm5, %ymm9 # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	192(%rsp), %ymm5, %ymm9 # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	%ymm1, %ymm5, %ymm9     # ymm9 = (ymm5 * ymm9) + ymm1
	vmulps	%ymm5, %ymm9, %ymm5
	vxorps	%xmm9, %xmm9, %xmm9
	vcmpltps	%ymm9, %ymm4, %ymm4
	vpsrad	$23, %ymm8, %ymm8
	vpaddd	736(%rsp), %ymm8, %ymm8         # 32-byte Folded Reload
	vcvtdq2ps	%ymm8, %ymm8
	vmovups	512(%rsp), %ymm3                # 32-byte Reload
	vblendvps	%ymm4, %ymm14, %ymm3, %ymm4
	vfmsub231ps	%ymm8, %ymm15, %ymm5    # ymm5 = (ymm15 * ymm8) - ymm5
	vblendvps	%ymm6, %ymm4, %ymm5, %ymm4
	vaddps	%ymm4, %ymm4, %ymm4
	vmulps	160(%rsp), %ymm4, %ymm5         # 32-byte Folded Reload
	vroundps	$9, %ymm5, %ymm5
	vcvttps2dq	%ymm5, %ymm6
	vfmsub231ps	128(%rsp), %ymm5, %ymm4 # 32-byte Folded Reload
                                        # ymm4 = (ymm5 * mem) - ymm4
	vfnmsub231ps	%ymm5, %ymm0, %ymm4     # ymm4 = -(ymm0 * ymm5) - ymm4
	vfmadd213ps	96(%rsp), %ymm4, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm4 * ymm10) + mem
	vfmadd213ps	320(%rsp), %ymm4, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm4 * ymm10) + mem
	vfmadd213ps	288(%rsp), %ymm4, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm4 * ymm10) + mem
	vfmadd213ps	%ymm11, %ymm4, %ymm10   # ymm10 = (ymm4 * ymm10) + ymm11
	vfmadd213ps	%ymm13, %ymm4, %ymm10   # ymm10 = (ymm4 * ymm10) + ymm13
	vfmadd213ps	%ymm1, %ymm4, %ymm10    # ymm10 = (ymm4 * ymm10) + ymm1
	vfmadd213ps	%ymm1, %ymm4, %ymm10    # ymm10 = (ymm4 * ymm10) + ymm1
	vpaddd	%ymm2, %ymm6, %ymm4
	vpslld	$23, %ymm4, %ymm8
	vmulps	%ymm8, %ymm10, %ymm5
	vpcmpgtd	%ymm2, %ymm6, %ymm6
	vmovups	864(%rsp), %ymm2                # 32-byte Reload
	vblendvps	%ymm6, 384(%rsp), %ymm5, %ymm5 # 32-byte Folded Reload
	vmovups	832(%rsp), %ymm6                # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm12, %ymm4
	vpandn	%ymm5, %ymm4, %ymm4
	vaddps	%ymm4, %ymm7, %ymm3
	vmovups	(%r9,%r15), %ymm4
	vfnmadd213ps	%ymm3, %ymm4, %ymm4     # ymm4 = -(ymm4 * ymm4) + ymm3
	vmulps	%ymm4, %ymm6, %ymm3
	vfmsub231ps	%ymm2, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm2) - ymm3
	vxorps	%xmm0, %xmm0, %xmm0
	vcmpltps	%ymm3, %ymm9, %ymm4
	vmovmskps	%ymm4, %r10d
	testb	%r10b, %r10b
	je	.LBB1_6
# %bb.8:                                # %safe_if_run_true
                                        #   in Loop: Header=BB1_5 Depth=1
	vbroadcastss	__real@80000000(%rip), %ymm5 # ymm5 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%ymm5, %ymm2, %ymm2
	vsqrtps	%ymm3, %ymm5
	vsubps	%ymm5, %ymm2, %ymm2
	vdivps	%ymm6, %ymm2, %ymm2
	vcmpltps	%ymm2, %ymm0, %ymm5
	vandps	%ymm4, %ymm5, %ymm4
	vextractf128	$1, %ymm4, %xmm5
	vpackssdw	%xmm5, %xmm4, %xmm4
	vpxor	__xmm@ffffffffffffffffffffffffffffffff(%rip), %xmm4, %xmm5
	vpacksswb	%xmm5, %xmm5, %xmm5
	vpmovmskb	%xmm5, %ebx
	cmpb	$-1, %bl
	je	.LBB1_10
# %bb.9:                                # %eval_1
                                        #   in Loop: Header=BB1_5 Depth=1
	vpmovsxwd	%xmm4, %ymm5
	vmaskmovps	(%r12,%r15), %ymm5, %ymm5
	vcmpltps	%ymm5, %ymm2, %ymm5
	vextractf128	$1, %ymm5, %xmm6
	vpackssdw	%xmm6, %xmm5, %xmm5
	vpand	%xmm4, %xmm5, %xmm4
.LBB1_10:                               # %logical_op_done
                                        #   in Loop: Header=BB1_5 Depth=1
	vpsllw	$15, %xmm4, %xmm5
	vpmovmskb	%xmm5, %ebx
	testl	$43690, %ebx                    # imm = 0xAAAA
	je	.LBB1_6
# %bb.11:                               # %safe_if_run_true183
                                        #   in Loop: Header=BB1_5 Depth=1
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vmaskmovps	%ymm2, %ymm4, (%r12,%r15)
.LBB1_6:                                # %safe_if_after_true
                                        #   in Loop: Header=BB1_5 Depth=1
	vcmpnltps	%ymm3, %ymm0, %ymm2
	vextractf128	$1, %ymm2, %xmm3
	vpackssdw	%xmm3, %xmm2, %xmm2
	vpmovmskb	%xmm2, %ebx
	testw	%bx, %bx
	je	.LBB1_7
# %bb.12:                               # %safe_if_run_false207
                                        #   in Loop: Header=BB1_5 Depth=1
	vpmovsxwd	%xmm2, %ymm2
	vbroadcastss	__real@4b189680(%rip), %ymm3 # ymm3 = [1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7]
	vmaskmovps	%ymm3, %ymm2, (%r12,%r15)
	jmp	.LBB1_7
.LBB1_1:
	xorl	%edi, %edi
.LBB1_2:                                # %partial_inner_all_outer
	movl	1368(%rsp), %eax
	cmpl	%eax, %edi
	jge	.LBB1_3
# %bb.13:                               # %partial_inner_only
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	__ymm@0000000700000006000000050000000400000003000000020000000100000000(%rip), %ymm0, %ymm0
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpcmpgtd	%ymm0, %ymm1, %ymm9
	shll	$2, %edi
	movslq	%edi, %rbp
	vmaskmovps	(%rsi,%rbp), %ymm9, %ymm1
	vpxor	%xmm0, %xmm0, %xmm0
	vcmpleps	%ymm0, %ymm1, %ymm3
	vxorps	%xmm11, %xmm11, %xmm11
	vbroadcastss	__real@3f800000(%rip), %ymm4 # ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vblendvps	%ymm3, %ymm4, %ymm1, %ymm0
	vmovaps	%ymm1, %ymm8
	vmovups	%ymm1, 736(%rsp)                # 32-byte Spill
	vbroadcastss	__real@807fffff(%rip), %ymm14 # ymm14 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vandps	%ymm0, %ymm14, %ymm1
	vbroadcastss	__real@3f000000(%rip), %ymm13 # ymm13 = [1056964608,1056964608,1056964608,1056964608,1056964608,1056964608,1056964608,1056964608]
	vorps	%ymm1, %ymm13, %ymm1
	vmovups	%ymm13, 352(%rsp)               # 32-byte Spill
	vsubps	%ymm1, %ymm4, %ymm1
	vbroadcastss	__real@7fc00000(%rip), %ymm2 # ymm2 = [NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN]
	vblendvps	%ymm9, %ymm1, %ymm2, %ymm6
	vbroadcastss	__real@3fdfe28e(%rip), %ymm5 # ymm5 = [1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0]
	vbroadcastss	__real@c01f5af7(%rip), %ymm1 # ymm1 = [-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0]
	vmovaps	%ymm5, %ymm7
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vmovaps	%ymm1, %ymm15
	vbroadcastss	__real@3ffe0193(%rip), %ymm12 # ymm12 = [1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0]
	vfmadd213ps	%ymm12, %ymm6, %ymm7    # ymm7 = (ymm6 * ymm7) + ymm12
	vmovups	%ymm12, 192(%rsp)               # 32-byte Spill
	vbroadcastss	__real@bf198181(%rip), %ymm1 # ymm1 = [-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1]
	vmovups	%ymm1, 160(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vbroadcastss	__real@3eaf548d(%rip), %ymm1 # ymm1 = [3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1]
	vmovups	%ymm1, 128(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vbroadcastss	__real@3e33a0af(%rip), %ymm1 # ymm1 = [1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1]
	vmovups	%ymm1, 96(%rsp)                 # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vbroadcastss	__real@3e80fb87(%rip), %ymm1 # ymm1 = [2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1]
	vmovups	%ymm1, 32(%rsp)                 # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vbroadcastss	__real@3eaaa11c(%rip), %ymm1 # ymm1 = [3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1]
	vmovups	%ymm1, (%rsp)                   # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vbroadcastss	__real@3f000010(%rip), %ymm1 # ymm1 = [5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1]
	vmovups	%ymm1, 64(%rsp)                 # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vfmadd213ps	%ymm4, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm4
	vmulps	%ymm7, %ymm6, %ymm7
	vpsrad	$23, %ymm0, %ymm0
	vpbroadcastd	__real@ffffff82(%rip), %ymm1 # ymm1 = [4294967170,4294967170,4294967170,4294967170,4294967170,4294967170,4294967170,4294967170]
	vmovdqu	%ymm1, 256(%rsp)                # 32-byte Spill
	vpaddd	%ymm1, %ymm0, %ymm0
	vcvtdq2ps	%ymm0, %ymm0
	vbroadcastss	__real@3f317218(%rip), %ymm1 # ymm1 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	vfmsub231ps	%ymm0, %ymm1, %ymm7     # ymm7 = (ymm1 * ymm0) - ymm7
	vmovups	%ymm1, 320(%rsp)                # 32-byte Spill
	vcmpltps	%ymm11, %ymm8, %ymm8
	vbroadcastss	__real@ff800000(%rip), %ymm0 # ymm0 = [-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf]
	vblendvps	%ymm8, %ymm2, %ymm0, %ymm8
	vmovaps	%ymm0, %ymm6
	vmovups	%ymm0, 224(%rsp)                # 32-byte Spill
	vblendvps	%ymm3, %ymm8, %ymm7, %ymm0
	vmovups	%ymm0, 704(%rsp)                # 32-byte Spill
	vmaskmovps	(%r11,%rbp), %ymm9, %ymm0
	vcmpleps	%ymm11, %ymm0, %ymm3
	vblendvps	%ymm3, %ymm4, %ymm0, %ymm7
	vmovaps	%ymm0, %ymm11
	vmovups	%ymm0, 672(%rsp)                # 32-byte Spill
	vmovaps	%ymm14, %ymm0
	vmovups	%ymm14, 288(%rsp)               # 32-byte Spill
	vandps	%ymm7, %ymm14, %ymm8
	vorps	%ymm13, %ymm8, %ymm8
	vsubps	%ymm8, %ymm4, %ymm8
	vblendvps	%ymm9, %ymm8, %ymm2, %ymm8
	vmovaps	%ymm5, %ymm13
	vmovaps	%ymm15, %ymm10
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vfmadd213ps	%ymm12, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm12
	vmovups	160(%rsp), %ymm12               # 32-byte Reload
	vfmadd213ps	%ymm12, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm12
	vmovups	128(%rsp), %ymm14               # 32-byte Reload
	vfmadd213ps	%ymm14, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm14
	vmovups	96(%rsp), %ymm15                # 32-byte Reload
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vfmadd213ps	32(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	(%rsp), %ymm8, %ymm13   # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	64(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	%ymm4, %ymm8, %ymm13    # ymm13 = (ymm8 * ymm13) + ymm4
	vmulps	%ymm13, %ymm8, %ymm8
	vpsrad	$23, %ymm7, %ymm7
	vpaddd	256(%rsp), %ymm7, %ymm7         # 32-byte Folded Reload
	vcvtdq2ps	%ymm7, %ymm7
	vfmsub231ps	%ymm7, %ymm1, %ymm8     # ymm8 = (ymm1 * ymm7) - ymm8
	vxorps	%xmm13, %xmm13, %xmm13
	vcmpltps	%ymm13, %ymm11, %ymm7
	vblendvps	%ymm7, %ymm2, %ymm6, %ymm7
	vblendvps	%ymm3, %ymm7, %ymm8, %ymm1
	vmovups	%ymm1, 640(%rsp)                # 32-byte Spill
	vmaskmovps	(%r13,%rbp), %ymm9, %ymm1
	vcmpleps	%ymm13, %ymm1, %ymm3
	vblendvps	%ymm3, %ymm4, %ymm1, %ymm7
	vmovups	%ymm1, 608(%rsp)                # 32-byte Spill
	vandps	%ymm0, %ymm7, %ymm8
	vmovups	352(%rsp), %ymm6                # 32-byte Reload
	vorps	%ymm6, %ymm8, %ymm8
	vsubps	%ymm8, %ymm4, %ymm8
	vblendvps	%ymm9, %ymm8, %ymm2, %ymm8
	vmovaps	%ymm5, %ymm13
	vfmadd213ps	%ymm10, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm10
	vmovaps	%ymm10, %ymm0
	vmovups	192(%rsp), %ymm10               # 32-byte Reload
	vfmadd213ps	%ymm10, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm10
	vmovaps	%ymm12, %ymm11
	vfmadd213ps	%ymm12, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm12
	vmovaps	%ymm14, %ymm12
	vfmadd213ps	%ymm14, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm14
	vmovaps	%ymm15, %ymm14
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vmovups	32(%rsp), %ymm15                # 32-byte Reload
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vfmadd213ps	(%rsp), %ymm8, %ymm13   # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	64(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	%ymm4, %ymm8, %ymm13    # ymm13 = (ymm8 * ymm13) + ymm4
	vmulps	%ymm13, %ymm8, %ymm8
	vpsrad	$23, %ymm7, %ymm7
	vpaddd	256(%rsp), %ymm7, %ymm7         # 32-byte Folded Reload
	vcvtdq2ps	%ymm7, %ymm7
	vfmsub231ps	320(%rsp), %ymm7, %ymm8 # 32-byte Folded Reload
                                        # ymm8 = (ymm7 * mem) - ymm8
	vxorps	%xmm13, %xmm13, %xmm13
	vcmpltps	%ymm13, %ymm1, %ymm7
	vmovups	224(%rsp), %ymm1                # 32-byte Reload
	vblendvps	%ymm7, %ymm2, %ymm1, %ymm7
	vblendvps	%ymm3, %ymm7, %ymm8, %ymm1
	vmovups	%ymm1, 576(%rsp)                # 32-byte Spill
	vmaskmovps	(%r14,%rbp), %ymm9, %ymm3
	vmaskmovps	(%rcx,%rbp), %ymm9, %ymm7
	vsubps	%ymm7, %ymm3, %ymm1
	vcmpleps	%ymm13, %ymm1, %ymm3
	vblendvps	%ymm3, %ymm4, %ymm1, %ymm7
	vmovups	%ymm1, 544(%rsp)                # 32-byte Spill
	vandps	288(%rsp), %ymm7, %ymm8         # 32-byte Folded Reload
	vorps	%ymm6, %ymm8, %ymm8
	vsubps	%ymm8, %ymm4, %ymm8
	vblendvps	%ymm9, %ymm8, %ymm2, %ymm8
	vmovaps	%ymm5, %ymm13
	vmovups	%ymm0, 768(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm0, %ymm8, %ymm13    # ymm13 = (ymm8 * ymm13) + ymm0
	vfmadd213ps	%ymm10, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm10
	vfmadd213ps	%ymm11, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm11
	vfmadd213ps	%ymm12, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm12
	vfmadd213ps	%ymm14, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm14
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vfmadd213ps	(%rsp), %ymm8, %ymm13   # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vmovups	64(%rsp), %ymm10                # 32-byte Reload
	vfmadd213ps	%ymm10, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm10
	vfmadd213ps	%ymm4, %ymm8, %ymm13    # ymm13 = (ymm8 * ymm13) + ymm4
	vmulps	%ymm13, %ymm8, %ymm8
	vpsrad	$23, %ymm7, %ymm7
	vmovdqu	256(%rsp), %ymm6                # 32-byte Reload
	vpaddd	%ymm6, %ymm7, %ymm7
	vcvtdq2ps	%ymm7, %ymm7
	vmovups	320(%rsp), %ymm11               # 32-byte Reload
	vfmsub231ps	%ymm7, %ymm11, %ymm8    # ymm8 = (ymm11 * ymm7) - ymm8
	vcmpltps	__ymm@0000000000000000000000000000000000000000000000000000000000000000(%rip), %ymm1, %ymm7
	vmovups	224(%rsp), %ymm0                # 32-byte Reload
	vblendvps	%ymm7, %ymm2, %ymm0, %ymm7
	vmovaps	%ymm2, %ymm13
	vblendvps	%ymm3, %ymm7, %ymm8, %ymm1
	vmovups	%ymm1, 512(%rsp)                # 32-byte Spill
	movq	1320(%rsp), %rax
	vmaskmovps	(%rax,%rbp), %ymm9, %ymm3
	vmaskmovps	(%rdx,%rbp), %ymm9, %ymm7
	vmovaps	%ymm9, %ymm2
	vsubps	%ymm7, %ymm3, %ymm1
	vxorps	%xmm3, %xmm3, %xmm3
	vcmpleps	%ymm3, %ymm1, %ymm3
	vblendvps	%ymm3, %ymm4, %ymm1, %ymm7
	vmovups	%ymm1, 480(%rsp)                # 32-byte Spill
	vmovups	288(%rsp), %ymm12               # 32-byte Reload
	vandps	%ymm7, %ymm12, %ymm8
	vmovups	352(%rsp), %ymm14               # 32-byte Reload
	vorps	%ymm14, %ymm8, %ymm8
	vsubps	%ymm8, %ymm4, %ymm8
	vmovaps	%ymm13, %ymm9
	vblendvps	%ymm2, %ymm8, %ymm13, %ymm8
	vmovaps	%ymm5, %ymm13
	vmovups	768(%rsp), %ymm15               # 32-byte Reload
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vfmadd213ps	192(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	160(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	128(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	96(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	32(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	(%rsp), %ymm8, %ymm13   # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	%ymm10, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm10
	vfmadd213ps	%ymm4, %ymm8, %ymm13    # ymm13 = (ymm8 * ymm13) + ymm4
	vmulps	%ymm13, %ymm8, %ymm8
	vpsrad	$23, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm7, %ymm7
	vcmpltps	__ymm@0000000000000000000000000000000000000000000000000000000000000000(%rip), %ymm1, %ymm13
	vblendvps	%ymm13, %ymm9, %ymm0, %ymm13
	vcvtdq2ps	%ymm7, %ymm7
	movq	1328(%rsp), %rax
	vmaskmovps	(%rax,%rbp), %ymm2, %ymm0
	vfmsub231ps	%ymm7, %ymm11, %ymm8    # ymm8 = (ymm11 * ymm7) - ymm8
	vmaskmovps	(%r8,%rbp), %ymm2, %ymm7
	vmovups	%ymm2, 416(%rsp)                # 32-byte Spill
	vblendvps	%ymm3, %ymm13, %ymm8, %ymm1
	vmovups	%ymm1, 448(%rsp)                # 32-byte Spill
	vsubps	%ymm7, %ymm0, %ymm1
	vxorps	%xmm8, %xmm8, %xmm8
	vcmpleps	%ymm8, %ymm1, %ymm0
	vmovaps	%ymm4, %ymm10
	vblendvps	%ymm0, %ymm4, %ymm1, %ymm3
	vmovups	%ymm1, 384(%rsp)                # 32-byte Spill
	vandps	%ymm3, %ymm12, %ymm7
	vorps	%ymm7, %ymm14, %ymm7
	vpsrad	$23, %ymm3, %ymm3
	vsubps	%ymm7, %ymm4, %ymm7
	vblendvps	%ymm2, %ymm7, %ymm9, %ymm7
	vpaddd	%ymm6, %ymm3, %ymm3
	vfmadd213ps	%ymm15, %ymm7, %ymm5    # ymm5 = (ymm7 * ymm5) + ymm15
	vfmadd213ps	192(%rsp), %ymm7, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	160(%rsp), %ymm7, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	128(%rsp), %ymm7, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	96(%rsp), %ymm7, %ymm5  # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	32(%rsp), %ymm7, %ymm5  # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	(%rsp), %ymm7, %ymm5    # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	64(%rsp), %ymm7, %ymm5  # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	%ymm4, %ymm7, %ymm5     # ymm5 = (ymm7 * ymm5) + ymm4
	vmulps	%ymm5, %ymm7, %ymm5
	vcvtdq2ps	%ymm3, %ymm3
	vfmsub231ps	%ymm3, %ymm11, %ymm5    # ymm5 = (ymm11 * ymm3) - ymm5
	vcmpltps	%ymm8, %ymm1, %ymm3
	vmovups	224(%rsp), %ymm1                # 32-byte Reload
	vblendvps	%ymm3, %ymm9, %ymm1, %ymm1
	vblendvps	%ymm0, %ymm1, %ymm5, %ymm0
	vmovups	%ymm0, 224(%rsp)                # 32-byte Spill
	vmovups	704(%rsp), %ymm0                # 32-byte Reload
	vaddps	%ymm0, %ymm0, %ymm3
	vbroadcastss	__real@3fb8aa3b(%rip), %ymm11 # ymm11 = [1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0]
	vmulps	%ymm3, %ymm11, %ymm0
	vroundps	$9, %ymm0, %ymm6
	vbroadcastss	__real@3f317200(%rip), %ymm5 # ymm5 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vfmsub231ps	%ymm5, %ymm6, %ymm3     # ymm3 = (ymm6 * ymm5) - ymm3
	vbroadcastss	__real@35bfbe8e(%rip), %ymm13 # ymm13 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vfnmsub231ps	%ymm6, %ymm13, %ymm3    # ymm3 = -(ymm13 * ymm6) - ymm3
	vmovups	%ymm13, 32(%rsp)                # 32-byte Spill
	vbroadcastss	__real@39907835(%rip), %ymm1 # ymm1 = [2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4]
	vbroadcastss	__real@3aaaf7b5(%rip), %ymm4 # ymm4 = [1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3]
	vmovaps	%ymm1, %ymm7
	vfmadd213ps	%ymm4, %ymm3, %ymm7     # ymm7 = (ymm3 * ymm7) + ymm4
	vmovups	%ymm4, (%rsp)                   # 32-byte Spill
	vbroadcastss	__real@3c09475d(%rip), %ymm14 # ymm14 = [8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3]
	vfmadd213ps	%ymm14, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm14
	vbroadcastss	__real@3d2a9d49(%rip), %ymm15 # ymm15 = [4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2]
	vfmadd213ps	%ymm15, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm15
	vbroadcastss	__real@3e2aab20(%rip), %ymm12 # ymm12 = [1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1]
	vfmadd213ps	%ymm12, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm12
	vbroadcastss	__real@3efffffd(%rip), %ymm9 # ymm9 = [4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1]
	vfmadd213ps	%ymm9, %ymm3, %ymm7     # ymm7 = (ymm3 * ymm7) + ymm9
	vmovaps	%ymm10, %ymm0
	vfmadd213ps	%ymm10, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm10
	vfmadd213ps	%ymm10, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm10
	vcvttps2dq	%ymm6, %ymm3
	vpbroadcastd	__real@0000007f(%rip), %ymm10 # ymm10 = [127,127,127,127,127,127,127,127]
	vpaddd	%ymm3, %ymm10, %ymm2
	vmovdqu	%ymm2, 64(%rsp)                 # 32-byte Spill
	vpslld	$23, %ymm2, %ymm8
	vmulps	%ymm7, %ymm8, %ymm8
	vpcmpgtd	%ymm10, %ymm3, %ymm3
	vbroadcastss	__real@7f800000(%rip), %ymm7 # ymm7 = [+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf]
	vblendvps	%ymm3, %ymm7, %ymm8, %ymm2
	vmovups	%ymm2, 256(%rsp)                # 32-byte Spill
	vmovups	640(%rsp), %ymm2                # 32-byte Reload
	vaddps	%ymm2, %ymm2, %ymm3
	vmulps	%ymm3, %ymm11, %ymm8
	vroundps	$9, %ymm8, %ymm8
	vfmsub231ps	%ymm5, %ymm8, %ymm3     # ymm3 = (ymm8 * ymm5) - ymm3
	vfnmsub231ps	%ymm8, %ymm13, %ymm3    # ymm3 = -(ymm13 * ymm8) - ymm3
	vmovaps	%ymm1, %ymm13
	vfmadd213ps	%ymm4, %ymm3, %ymm13    # ymm13 = (ymm3 * ymm13) + ymm4
	vfmadd213ps	%ymm14, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm13) + ymm14
	vfmadd213ps	%ymm15, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm13) + ymm15
	vfmadd213ps	%ymm12, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm13) + ymm12
	vfmadd213ps	%ymm9, %ymm3, %ymm13    # ymm13 = (ymm3 * ymm13) + ymm9
	vfmadd213ps	%ymm0, %ymm3, %ymm13    # ymm13 = (ymm3 * ymm13) + ymm0
	vfmadd213ps	%ymm0, %ymm3, %ymm13    # ymm13 = (ymm3 * ymm13) + ymm0
	vcvttps2dq	%ymm8, %ymm8
	vpaddd	%ymm10, %ymm8, %ymm2
	vmovdqu	%ymm2, 192(%rsp)                # 32-byte Spill
	vpslld	$23, %ymm2, %ymm6
	vmulps	%ymm6, %ymm13, %ymm6
	vpcmpgtd	%ymm10, %ymm8, %ymm8
	vblendvps	%ymm8, %ymm7, %ymm6, %ymm2
	vmovups	%ymm2, 160(%rsp)                # 32-byte Spill
	vmovups	576(%rsp), %ymm2                # 32-byte Reload
	vaddps	%ymm2, %ymm2, %ymm6
	vmulps	%ymm6, %ymm11, %ymm8
	vroundps	$9, %ymm8, %ymm8
	vfmsub231ps	%ymm5, %ymm8, %ymm6     # ymm6 = (ymm8 * ymm5) - ymm6
	vmovups	32(%rsp), %ymm3                 # 32-byte Reload
	vfnmsub231ps	%ymm8, %ymm3, %ymm6     # ymm6 = -(ymm3 * ymm8) - ymm6
	vmovaps	%ymm1, %ymm4
	vmovups	(%rsp), %ymm13                  # 32-byte Reload
	vfmadd213ps	%ymm13, %ymm6, %ymm4    # ymm4 = (ymm6 * ymm4) + ymm13
	vfmadd213ps	%ymm14, %ymm6, %ymm4    # ymm4 = (ymm6 * ymm4) + ymm14
	vfmadd213ps	%ymm15, %ymm6, %ymm4    # ymm4 = (ymm6 * ymm4) + ymm15
	vfmadd213ps	%ymm12, %ymm6, %ymm4    # ymm4 = (ymm6 * ymm4) + ymm12
	vfmadd213ps	%ymm9, %ymm6, %ymm4     # ymm4 = (ymm6 * ymm4) + ymm9
	vfmadd213ps	%ymm0, %ymm6, %ymm4     # ymm4 = (ymm6 * ymm4) + ymm0
	vfmadd213ps	%ymm0, %ymm6, %ymm4     # ymm4 = (ymm6 * ymm4) + ymm0
	vcvttps2dq	%ymm8, %ymm6
	vpaddd	%ymm6, %ymm10, %ymm2
	vmovdqu	%ymm2, 128(%rsp)                # 32-byte Spill
	vpslld	$23, %ymm2, %ymm8
	vmulps	%ymm4, %ymm8, %ymm4
	vpcmpgtd	%ymm10, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm7, %ymm4, %ymm2
	vmovups	%ymm2, 96(%rsp)                 # 32-byte Spill
	vmovups	512(%rsp), %ymm2                # 32-byte Reload
	vaddps	%ymm2, %ymm2, %ymm4
	vmulps	%ymm4, %ymm11, %ymm6
	vroundps	$9, %ymm6, %ymm6
	vfmsub231ps	%ymm5, %ymm6, %ymm4     # ymm4 = (ymm6 * ymm5) - ymm4
	vfnmsub231ps	%ymm6, %ymm3, %ymm4     # ymm4 = -(ymm3 * ymm6) - ymm4
	vmovaps	%ymm1, %ymm8
	vfmadd213ps	%ymm13, %ymm4, %ymm8    # ymm8 = (ymm4 * ymm8) + ymm13
	vfmadd213ps	%ymm14, %ymm4, %ymm8    # ymm8 = (ymm4 * ymm8) + ymm14
	vfmadd213ps	%ymm15, %ymm4, %ymm8    # ymm8 = (ymm4 * ymm8) + ymm15
	vfmadd213ps	%ymm12, %ymm4, %ymm8    # ymm8 = (ymm4 * ymm8) + ymm12
	vfmadd213ps	%ymm9, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm9
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vmovaps	%ymm0, %ymm2
	vcvttps2dq	%ymm6, %ymm4
	vpaddd	%ymm4, %ymm10, %ymm0
	vmovdqu	%ymm0, 288(%rsp)                # 32-byte Spill
	vpslld	$23, %ymm0, %ymm3
	vmulps	%ymm3, %ymm8, %ymm3
	vpcmpgtd	%ymm10, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm7, %ymm3, %ymm0
	vmovups	%ymm0, 320(%rsp)                # 32-byte Spill
	vmovups	480(%rsp), %ymm0                # 32-byte Reload
	vmulps	672(%rsp), %ymm0, %ymm8         # 32-byte Folded Reload
	vmovups	448(%rsp), %ymm0                # 32-byte Reload
	vaddps	%ymm0, %ymm0, %ymm4
	vmulps	%ymm4, %ymm11, %ymm3
	vroundps	$9, %ymm3, %ymm3
	vfmsub231ps	%ymm5, %ymm3, %ymm4     # ymm4 = (ymm3 * ymm5) - ymm4
	vmovups	32(%rsp), %ymm6                 # 32-byte Reload
	vfnmsub231ps	%ymm3, %ymm6, %ymm4     # ymm4 = -(ymm6 * ymm3) - ymm4
	vmovaps	%ymm1, %ymm0
	vfmadd213ps	%ymm13, %ymm4, %ymm0    # ymm0 = (ymm4 * ymm0) + ymm13
	vfmadd213ps	%ymm14, %ymm4, %ymm0    # ymm0 = (ymm4 * ymm0) + ymm14
	vfmadd213ps	%ymm15, %ymm4, %ymm0    # ymm0 = (ymm4 * ymm0) + ymm15
	vfmadd213ps	%ymm12, %ymm4, %ymm0    # ymm0 = (ymm4 * ymm0) + ymm12
	vfmadd213ps	%ymm9, %ymm4, %ymm0     # ymm0 = (ymm4 * ymm0) + ymm9
	vfmadd213ps	%ymm2, %ymm4, %ymm0     # ymm0 = (ymm4 * ymm0) + ymm2
	vfmadd213ps	%ymm2, %ymm4, %ymm0     # ymm0 = (ymm4 * ymm0) + ymm2
	vcvttps2dq	%ymm3, %ymm3
	vpaddd	%ymm3, %ymm10, %ymm4
	vpslld	$23, %ymm4, %ymm13
	vmulps	%ymm0, %ymm13, %ymm0
	vpcmpgtd	%ymm10, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm7, %ymm0, %ymm0
	vmovups	544(%rsp), %ymm3                # 32-byte Reload
	vfmadd231ps	736(%rsp), %ymm3, %ymm8 # 32-byte Folded Reload
                                        # ymm8 = (ymm3 * mem) + ymm8
	vmovups	224(%rsp), %ymm3                # 32-byte Reload
	vaddps	%ymm3, %ymm3, %ymm3
	vmulps	%ymm3, %ymm11, %ymm11
	vroundps	$9, %ymm11, %ymm11
	vfmsub213ps	%ymm3, %ymm11, %ymm5    # ymm5 = (ymm11 * ymm5) - ymm3
	vfnmsub231ps	%ymm6, %ymm11, %ymm5    # ymm5 = -(ymm11 * ymm6) - ymm5
	vfmadd213ps	(%rsp), %ymm5, %ymm1    # 32-byte Folded Reload
                                        # ymm1 = (ymm5 * ymm1) + mem
	vfmadd213ps	%ymm14, %ymm5, %ymm1    # ymm1 = (ymm5 * ymm1) + ymm14
	vfmadd213ps	%ymm15, %ymm5, %ymm1    # ymm1 = (ymm5 * ymm1) + ymm15
	vfmadd213ps	%ymm12, %ymm5, %ymm1    # ymm1 = (ymm5 * ymm1) + ymm12
	vfmadd213ps	%ymm9, %ymm5, %ymm1     # ymm1 = (ymm5 * ymm1) + ymm9
	vfmadd213ps	%ymm2, %ymm5, %ymm1     # ymm1 = (ymm5 * ymm1) + ymm2
	vfmadd213ps	%ymm2, %ymm5, %ymm1     # ymm1 = (ymm5 * ymm1) + ymm2
	vcvttps2dq	%ymm11, %ymm2
	vpaddd	%ymm2, %ymm10, %ymm3
	vpcmpgtd	%ymm10, %ymm2, %ymm2
	vpslld	$23, %ymm3, %ymm5
	vmulps	%ymm5, %ymm1, %ymm1
	vblendvps	%ymm2, %ymm7, %ymm1, %ymm2
	vmovups	384(%rsp), %ymm1                # 32-byte Reload
	vfmadd231ps	608(%rsp), %ymm1, %ymm8 # 32-byte Folded Reload
                                        # ymm8 = (ymm1 * mem) + ymm8
	vpbroadcastd	__real@00000001(%rip), %ymm5 # ymm5 = [1,1,1,1,1,1,1,1]
	vpcmpgtd	64(%rsp), %ymm5, %ymm1          # 32-byte Folded Reload
	vpandn	256(%rsp), %ymm1, %ymm1         # 32-byte Folded Reload
	vpcmpgtd	192(%rsp), %ymm5, %ymm7         # 32-byte Folded Reload
	vpandn	160(%rsp), %ymm7, %ymm7         # 32-byte Folded Reload
	vaddps	%ymm7, %ymm1, %ymm1
	vpcmpgtd	128(%rsp), %ymm5, %ymm7         # 32-byte Folded Reload
	vpandn	96(%rsp), %ymm7, %ymm7          # 32-byte Folded Reload
	vaddps	%ymm7, %ymm1, %ymm1
	vpcmpgtd	288(%rsp), %ymm5, %ymm6         # 32-byte Folded Reload
	vpandn	320(%rsp), %ymm6, %ymm6         # 32-byte Folded Reload
	vpcmpgtd	%ymm4, %ymm5, %ymm4
	vpandn	%ymm0, %ymm4, %ymm0
	vaddps	%ymm0, %ymm6, %ymm0
	vxorps	%xmm6, %xmm6, %xmm6
	vpcmpgtd	%ymm3, %ymm5, %ymm3
	vmovups	416(%rsp), %ymm5                # 32-byte Reload
	vpandn	%ymm2, %ymm3, %ymm2
	vmaskmovps	(%r9,%rbp), %ymm5, %ymm3
	vaddps	%ymm2, %ymm0, %ymm0
	vfnmadd213ps	%ymm0, %ymm3, %ymm3     # ymm3 = -(ymm3 * ymm3) + ymm0
	vmulps	%ymm3, %ymm1, %ymm0
	vfmsub231ps	%ymm8, %ymm8, %ymm0     # ymm0 = (ymm8 * ymm8) - ymm0
	vcmpltps	%ymm0, %ymm6, %ymm2
	vandps	%ymm5, %ymm2, %ymm2
	vmovmskps	%ymm2, %eax
	testb	%al, %al
	je	.LBB1_14
# %bb.16:                               # %safe_if_run_true397
	vextractf128	$1, %ymm2, %xmm3
	vpackssdw	%xmm3, %xmm2, %xmm2
	vbroadcastss	__real@80000000(%rip), %ymm3 # ymm3 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%ymm3, %ymm8, %ymm3
	vsqrtps	%ymm0, %ymm4
	vsubps	%ymm4, %ymm3, %ymm3
	vdivps	%ymm1, %ymm3, %ymm1
	vcmpltps	%ymm1, %ymm6, %ymm3
	vextractf128	$1, %ymm3, %xmm4
	vpackssdw	%xmm4, %xmm3, %xmm3
	vpand	%xmm2, %xmm3, %xmm2
	vpcmpeqd	%xmm3, %xmm3, %xmm3
	vpxor	%xmm3, %xmm2, %xmm3
	vpacksswb	%xmm3, %xmm3, %xmm3
	vpmovmskb	%xmm3, %eax
	cmpb	$-1, %al
	je	.LBB1_18
# %bb.17:                               # %eval_1412
	vpmovsxwd	%xmm2, %ymm3
	vmaskmovps	(%r12,%rbp), %ymm3, %ymm3
	vcmpltps	%ymm3, %ymm1, %ymm3
	vextractf128	$1, %ymm3, %xmm4
	vpackssdw	%xmm4, %xmm3, %xmm3
	vpand	%xmm2, %xmm3, %xmm2
.LBB1_18:                               # %logical_op_done413
	vpsllw	$15, %xmm2, %xmm3
	vpmovmskb	%xmm3, %eax
	testl	$43690, %eax                    # imm = 0xAAAA
	je	.LBB1_14
# %bb.19:                               # %safe_if_run_true441
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vmaskmovps	%ymm1, %ymm2, (%r12,%rbp)
.LBB1_14:                               # %safe_if_after_true396
	vcmpnltps	%ymm0, %ymm6, %ymm0
	vandps	%ymm0, %ymm5, %ymm0
	vextractf128	$1, %ymm0, %xmm1
	vpackssdw	%xmm1, %xmm0, %xmm0
	vpmovmskb	%xmm0, %eax
	testw	%ax, %ax
	je	.LBB1_3
# %bb.15:                               # %safe_if_run_false473
	vpmovsxwd	%xmm0, %ymm0
	vbroadcastss	__real@4b189680(%rip), %ymm1 # ymm1 = [1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7]
	vmaskmovps	%ymm1, %ymm0, (%r12,%rbp)
.LBB1_3:                                # %foreach_reset
	vmovaps	1040(%rsp), %xmm6               # 16-byte Reload
	vmovaps	1056(%rsp), %xmm7               # 16-byte Reload
	vmovaps	1072(%rsp), %xmm8               # 16-byte Reload
	vmovaps	1088(%rsp), %xmm9               # 16-byte Reload
	vmovaps	1104(%rsp), %xmm10              # 16-byte Reload
	vmovaps	1120(%rsp), %xmm11              # 16-byte Reload
	vmovaps	1136(%rsp), %xmm12              # 16-byte Reload
	vmovaps	1152(%rsp), %xmm13              # 16-byte Reload
	vmovaps	1168(%rsp), %xmm14              # 16-byte Reload
	vmovaps	1184(%rsp), %xmm15              # 16-byte Reload
	addq	$1208, %rsp                     # imm = 0x4B8
	popq	%rbx
	popq	%rbp
	popq	%rdi
	popq	%rsi
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	vzeroupper
	retq
                                        # -- End function
	.def	 SphereHitN;
	.scl	2;
	.type	32;
	.endef
	.globl	SphereHitN                      # -- Begin function SphereHitN
	.p2align	4, 0x90
SphereHitN:                             # @SphereHitN
# %bb.0:                                # %allocas
	pushq	%r15
	pushq	%r14
	pushq	%r13
	pushq	%r12
	pushq	%rsi
	pushq	%rdi
	pushq	%rbp
	pushq	%rbx
	subq	$1208, %rsp                     # imm = 0x4B8
	vmovaps	%xmm15, 1184(%rsp)              # 16-byte Spill
	vmovaps	%xmm14, 1168(%rsp)              # 16-byte Spill
	vmovaps	%xmm13, 1152(%rsp)              # 16-byte Spill
	vmovdqa	%xmm12, 1136(%rsp)              # 16-byte Spill
	vmovaps	%xmm11, 1120(%rsp)              # 16-byte Spill
	vmovaps	%xmm10, 1104(%rsp)              # 16-byte Spill
	vmovaps	%xmm9, 1088(%rsp)               # 16-byte Spill
	vmovaps	%xmm8, 1072(%rsp)               # 16-byte Spill
	vmovaps	%xmm7, 1056(%rsp)               # 16-byte Spill
	vmovaps	%xmm6, 1040(%rsp)               # 16-byte Spill
	movl	1368(%rsp), %eax
	movq	1360(%rsp), %r12
	movq	1352(%rsp), %r13
	movq	1344(%rsp), %r11
	movq	1336(%rsp), %rsi
	movq	1312(%rsp), %r14
	leal	7(%rax), %ebp
	testl	%eax, %eax
	cmovnsl	%eax, %ebp
	andl	$-8, %ebp
	testl	%ebp, %ebp
	jle	.LBB2_1
# %bb.4:                                # %foreach_full_body.lr.ph
	xorl	%eax, %eax
	vbroadcastss	__real@3f800000(%rip), %ymm1 # ymm1 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vbroadcastss	__real@ffffff82(%rip), %ymm2 # ymm2 = [4294967170,4294967170,4294967170,4294967170,4294967170,4294967170,4294967170,4294967170]
	vmovups	%ymm2, 736(%rsp)                # 32-byte Spill
	vbroadcastss	__real@807fffff(%rip), %ymm2 # ymm2 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vmovups	%ymm2, 704(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3f000000(%rip), %ymm2 # ymm2 = [1056964608,1056964608,1056964608,1056964608,1056964608,1056964608,1056964608,1056964608]
	vmovups	%ymm2, 256(%rsp)                # 32-byte Spill
	vpbroadcastd	__real@3fdfe28e(%rip), %ymm2 # ymm2 = [1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0]
	vmovdqu	%ymm2, 224(%rsp)                # 32-byte Spill
	vbroadcastss	__real@c01f5af7(%rip), %ymm0 # ymm0 = [-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0]
	vmovups	%ymm0, (%rsp)                   # 32-byte Spill
	vbroadcastss	__real@3ffe0193(%rip), %ymm0 # ymm0 = [1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0]
	vmovups	%ymm0, 32(%rsp)                 # 32-byte Spill
	vbroadcastss	__real@bf198181(%rip), %ymm0 # ymm0 = [-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1]
	vmovups	%ymm0, 64(%rsp)                 # 32-byte Spill
	vbroadcastss	__real@3eaf548d(%rip), %ymm0 # ymm0 = [3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1]
	vmovups	%ymm0, 768(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3e33a0af(%rip), %ymm0 # ymm0 = [1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1]
	vmovups	%ymm0, 672(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3e80fb87(%rip), %ymm0 # ymm0 = [2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1]
	vmovups	%ymm0, 640(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3eaaa11c(%rip), %ymm0 # ymm0 = [3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1]
	vmovups	%ymm0, 608(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3f000010(%rip), %ymm0 # ymm0 = [5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1]
	vmovups	%ymm0, 192(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3f317218(%rip), %ymm0 # ymm0 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	vmovups	%ymm0, 576(%rsp)                # 32-byte Spill
	vbroadcastss	__real@7fc00000(%rip), %ymm0 # ymm0 = [NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN]
	vmovups	%ymm0, 544(%rsp)                # 32-byte Spill
	xorl	%edi, %edi
	vbroadcastss	__real@ff800000(%rip), %ymm0 # ymm0 = [-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf]
	vmovups	%ymm0, 512(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3fb8aa3b(%rip), %ymm0 # ymm0 = [1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0]
	vmovups	%ymm0, 160(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3f317200(%rip), %ymm0 # ymm0 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vmovups	%ymm0, 128(%rsp)                # 32-byte Spill
	vbroadcastss	__real@35bfbe8e(%rip), %ymm0 # ymm0 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vmovups	%ymm0, 480(%rsp)                # 32-byte Spill
	vbroadcastss	__real@39907835(%rip), %ymm0 # ymm0 = [2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4]
	vmovups	%ymm0, 928(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3aaaf7b5(%rip), %ymm0 # ymm0 = [1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3]
	vmovups	%ymm0, 96(%rsp)                 # 32-byte Spill
	vbroadcastss	__real@3c09475d(%rip), %ymm0 # ymm0 = [8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3]
	vmovups	%ymm0, 320(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3d2a9d49(%rip), %ymm0 # ymm0 = [4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2]
	vmovups	%ymm0, 288(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3e2aab20(%rip), %ymm0 # ymm0 = [1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1]
	vmovups	%ymm0, 448(%rsp)                # 32-byte Spill
	vbroadcastss	__real@3efffffd(%rip), %ymm0 # ymm0 = [4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1]
	vmovups	%ymm0, 416(%rsp)                # 32-byte Spill
	vbroadcastss	__real@0000007f(%rip), %ymm0 # ymm0 = [127,127,127,127,127,127,127,127]
	vmovups	%ymm0, 352(%rsp)                # 32-byte Spill
	vbroadcastss	__real@00000001(%rip), %ymm0 # ymm0 = [1,1,1,1,1,1,1,1]
	vmovups	%ymm0, 896(%rsp)                # 32-byte Spill
	vbroadcastss	__real@7f800000(%rip), %ymm0 # ymm0 = [+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf]
	vmovups	%ymm0, 384(%rsp)                # 32-byte Spill
	jmp	.LBB2_5
	.p2align	4, 0x90
.LBB2_7:                                # %if_done
                                        #   in Loop: Header=BB2_5 Depth=1
	addl	$8, %edi
	addl	$32, %eax
	cmpl	%ebp, %edi
	jge	.LBB2_2
.LBB2_5:                                # %foreach_full_body
                                        # =>This Inner Loop Header: Depth=1
	movslq	%eax, %r15
	vmovups	(%rsi,%r15), %ymm4
	vmovups	%ymm4, 864(%rsp)                # 32-byte Spill
	vxorps	%xmm0, %xmm0, %xmm0
	vcmpltps	%ymm0, %ymm4, %ymm2
	vcmpleps	%ymm0, %ymm4, %ymm3
	vblendvps	%ymm3, %ymm1, %ymm4, %ymm4
	vpsrad	$23, %ymm4, %ymm6
	vmovdqu	736(%rsp), %ymm0                # 32-byte Reload
	vpaddd	%ymm0, %ymm6, %ymm6
	vmovdqa	%ymm0, %ymm10
	vmovups	704(%rsp), %ymm13               # 32-byte Reload
	vandps	%ymm4, %ymm13, %ymm4
	vorps	256(%rsp), %ymm4, %ymm4         # 32-byte Folded Reload
	vsubps	%ymm4, %ymm1, %ymm4
	vmovups	224(%rsp), %ymm11               # 32-byte Reload
	vmovaps	%ymm11, %ymm8
	vmovups	(%rsp), %ymm0                   # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	32(%rsp), %ymm0                 # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	64(%rsp), %ymm0                 # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vmovups	768(%rsp), %ymm7                # 32-byte Reload
	vfmadd213ps	%ymm7, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm7
	vmovups	672(%rsp), %ymm0                # 32-byte Reload
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vfmadd213ps	640(%rsp), %ymm4, %ymm8 # 32-byte Folded Reload
                                        # ymm8 = (ymm4 * ymm8) + mem
	vmovups	608(%rsp), %ymm5                # 32-byte Reload
	vfmadd213ps	%ymm5, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm5
	vmovups	192(%rsp), %ymm9                # 32-byte Reload
	vfmadd213ps	%ymm9, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm9
	vfmadd213ps	%ymm1, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm1
	vcvtdq2ps	%ymm6, %ymm6
	vmulps	%ymm4, %ymm8, %ymm4
	vfmsub231ps	576(%rsp), %ymm6, %ymm4 # 32-byte Folded Reload
                                        # ymm4 = (ymm6 * mem) - ymm4
	vmovups	512(%rsp), %ymm6                # 32-byte Reload
	vblendvps	%ymm2, 544(%rsp), %ymm6, %ymm2 # 32-byte Folded Reload
	vblendvps	%ymm3, %ymm2, %ymm4, %ymm2
	vaddps	%ymm2, %ymm2, %ymm2
	vmovups	160(%rsp), %ymm14               # 32-byte Reload
	vmulps	%ymm2, %ymm14, %ymm3
	vroundps	$9, %ymm3, %ymm3
	vcvttps2dq	%ymm3, %ymm4
	vfmsub231ps	128(%rsp), %ymm3, %ymm2 # 32-byte Folded Reload
                                        # ymm2 = (ymm3 * mem) - ymm2
	vfnmsub231ps	480(%rsp), %ymm3, %ymm2 # 32-byte Folded Reload
                                        # ymm2 = -(ymm3 * mem) - ymm2
	vmovups	928(%rsp), %ymm15               # 32-byte Reload
	vmovaps	%ymm15, %ymm3
	vmovups	96(%rsp), %ymm12                # 32-byte Reload
	vfmadd213ps	%ymm12, %ymm2, %ymm3    # ymm3 = (ymm2 * ymm3) + ymm12
	vfmadd213ps	320(%rsp), %ymm2, %ymm3 # 32-byte Folded Reload
                                        # ymm3 = (ymm2 * ymm3) + mem
	vmovups	288(%rsp), %ymm6                # 32-byte Reload
	vfmadd213ps	%ymm6, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm3) + ymm6
	vmovups	448(%rsp), %ymm6                # 32-byte Reload
	vfmadd213ps	%ymm6, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm3) + ymm6
	vmovups	416(%rsp), %ymm6                # 32-byte Reload
	vfmadd213ps	%ymm6, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm3) + ymm6
	vfmadd213ps	%ymm1, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm3) + ymm1
	vfmadd213ps	%ymm1, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm3) + ymm1
	vmovdqu	352(%rsp), %ymm6                # 32-byte Reload
	vpaddd	%ymm6, %ymm4, %ymm2
	vpcmpgtd	%ymm6, %ymm4, %ymm4
	vpslld	$23, %ymm2, %ymm6
	vmulps	%ymm6, %ymm3, %ymm3
	vblendvps	%ymm4, 384(%rsp), %ymm3, %ymm3 # 32-byte Folded Reload
	vmovdqu	896(%rsp), %ymm4                # 32-byte Reload
	vpcmpgtd	%ymm2, %ymm4, %ymm2
	vpandn	%ymm3, %ymm2, %ymm2
	vmovdqu	%ymm2, 832(%rsp)                # 32-byte Spill
	vmovups	(%r11,%r15), %ymm2
	vmovups	%ymm2, 800(%rsp)                # 32-byte Spill
	vxorps	%xmm3, %xmm3, %xmm3
	vcmpltps	%ymm3, %ymm2, %ymm4
	vcmpleps	%ymm3, %ymm2, %ymm6
	vblendvps	%ymm6, %ymm1, %ymm2, %ymm8
	vpsrad	$23, %ymm8, %ymm9
	vpaddd	%ymm10, %ymm9, %ymm9
	vmovaps	%ymm13, %ymm2
	vandps	%ymm13, %ymm8, %ymm8
	vmovups	256(%rsp), %ymm13               # 32-byte Reload
	vorps	%ymm13, %ymm8, %ymm8
	vsubps	%ymm8, %ymm1, %ymm8
	vmovaps	%ymm11, %ymm10
	vfmadd213ps	(%rsp), %ymm8, %ymm10   # 32-byte Folded Reload
                                        # ymm10 = (ymm8 * ymm10) + mem
	vfmadd213ps	32(%rsp), %ymm8, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm8 * ymm10) + mem
	vfmadd213ps	64(%rsp), %ymm8, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm8 * ymm10) + mem
	vfmadd213ps	%ymm7, %ymm8, %ymm10    # ymm10 = (ymm8 * ymm10) + ymm7
	vfmadd213ps	%ymm0, %ymm8, %ymm10    # ymm10 = (ymm8 * ymm10) + ymm0
	vmovups	640(%rsp), %ymm11               # 32-byte Reload
	vfmadd213ps	%ymm11, %ymm8, %ymm10   # ymm10 = (ymm8 * ymm10) + ymm11
	vfmadd213ps	%ymm5, %ymm8, %ymm10    # ymm10 = (ymm8 * ymm10) + ymm5
	vfmadd213ps	192(%rsp), %ymm8, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm8 * ymm10) + mem
	vfmadd213ps	%ymm1, %ymm8, %ymm10    # ymm10 = (ymm8 * ymm10) + ymm1
	vcvtdq2ps	%ymm9, %ymm9
	vmulps	%ymm10, %ymm8, %ymm8
	vmovups	544(%rsp), %ymm0                # 32-byte Reload
	vmovups	512(%rsp), %ymm7                # 32-byte Reload
	vblendvps	%ymm4, %ymm0, %ymm7, %ymm4
	vfmsub231ps	576(%rsp), %ymm9, %ymm8 # 32-byte Folded Reload
                                        # ymm8 = (ymm9 * mem) - ymm8
	vblendvps	%ymm6, %ymm4, %ymm8, %ymm4
	vaddps	%ymm4, %ymm4, %ymm4
	vmulps	%ymm4, %ymm14, %ymm6
	vroundps	$9, %ymm6, %ymm6
	vcvttps2dq	%ymm6, %ymm8
	vfmsub231ps	128(%rsp), %ymm6, %ymm4 # 32-byte Folded Reload
                                        # ymm4 = (ymm6 * mem) - ymm4
	vfnmsub231ps	480(%rsp), %ymm6, %ymm4 # 32-byte Folded Reload
                                        # ymm4 = -(ymm6 * mem) - ymm4
	vfmadd213ps	%ymm12, %ymm4, %ymm15   # ymm15 = (ymm4 * ymm15) + ymm12
	vfmadd213ps	320(%rsp), %ymm4, %ymm15 # 32-byte Folded Reload
                                        # ymm15 = (ymm4 * ymm15) + mem
	vfmadd213ps	288(%rsp), %ymm4, %ymm15 # 32-byte Folded Reload
                                        # ymm15 = (ymm4 * ymm15) + mem
	vfmadd213ps	448(%rsp), %ymm4, %ymm15 # 32-byte Folded Reload
                                        # ymm15 = (ymm4 * ymm15) + mem
	vfmadd213ps	416(%rsp), %ymm4, %ymm15 # 32-byte Folded Reload
                                        # ymm15 = (ymm4 * ymm15) + mem
	vfmadd213ps	%ymm1, %ymm4, %ymm15    # ymm15 = (ymm4 * ymm15) + ymm1
	vfmadd213ps	%ymm1, %ymm4, %ymm15    # ymm15 = (ymm4 * ymm15) + ymm1
	vmovdqu	352(%rsp), %ymm3                # 32-byte Reload
	vpaddd	%ymm3, %ymm8, %ymm4
	vpcmpgtd	%ymm3, %ymm8, %ymm8
	vpslld	$23, %ymm4, %ymm9
	vmulps	%ymm9, %ymm15, %ymm6
	vmovups	384(%rsp), %ymm15               # 32-byte Reload
	vblendvps	%ymm8, %ymm15, %ymm6, %ymm6
	vmovdqu	896(%rsp), %ymm3                # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm3, %ymm4
	vpandn	%ymm6, %ymm4, %ymm4
	vaddps	832(%rsp), %ymm4, %ymm4         # 32-byte Folded Reload
	vmovups	(%r13,%r15), %ymm9
	vmovups	%ymm9, 992(%rsp)                # 32-byte Spill
	vxorps	%xmm5, %xmm5, %xmm5
	vcmpltps	%ymm5, %ymm9, %ymm6
	vcmpleps	%ymm5, %ymm9, %ymm8
	vblendvps	%ymm8, %ymm1, %ymm9, %ymm9
	vpsrad	$23, %ymm9, %ymm10
	vpaddd	736(%rsp), %ymm10, %ymm10       # 32-byte Folded Reload
	vandps	%ymm2, %ymm9, %ymm9
	vorps	%ymm13, %ymm9, %ymm9
	vsubps	%ymm9, %ymm1, %ymm9
	vmovups	224(%rsp), %ymm5                # 32-byte Reload
	vfmadd213ps	(%rsp), %ymm9, %ymm5    # 32-byte Folded Reload
                                        # ymm5 = (ymm9 * ymm5) + mem
	vfmadd213ps	32(%rsp), %ymm9, %ymm5  # 32-byte Folded Reload
                                        # ymm5 = (ymm9 * ymm5) + mem
	vfmadd213ps	64(%rsp), %ymm9, %ymm5  # 32-byte Folded Reload
                                        # ymm5 = (ymm9 * ymm5) + mem
	vmovups	768(%rsp), %ymm14               # 32-byte Reload
	vfmadd213ps	%ymm14, %ymm9, %ymm5    # ymm5 = (ymm9 * ymm5) + ymm14
	vmovups	672(%rsp), %ymm12               # 32-byte Reload
	vfmadd213ps	%ymm12, %ymm9, %ymm5    # ymm5 = (ymm9 * ymm5) + ymm12
	vfmadd213ps	%ymm11, %ymm9, %ymm5    # ymm5 = (ymm9 * ymm5) + ymm11
	vmovups	608(%rsp), %ymm2                # 32-byte Reload
	vfmadd213ps	%ymm2, %ymm9, %ymm5     # ymm5 = (ymm9 * ymm5) + ymm2
	vfmadd213ps	192(%rsp), %ymm9, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm9 * ymm5) + mem
	vfmadd213ps	%ymm1, %ymm9, %ymm5     # ymm5 = (ymm9 * ymm5) + ymm1
	vcvtdq2ps	%ymm10, %ymm10
	vmulps	%ymm5, %ymm9, %ymm5
	vblendvps	%ymm6, %ymm0, %ymm7, %ymm6
	vmovups	576(%rsp), %ymm0                # 32-byte Reload
	vfmsub231ps	%ymm10, %ymm0, %ymm5    # ymm5 = (ymm0 * ymm10) - ymm5
	vblendvps	%ymm8, %ymm6, %ymm5, %ymm5
	vaddps	%ymm5, %ymm5, %ymm5
	vmulps	160(%rsp), %ymm5, %ymm6         # 32-byte Folded Reload
	vroundps	$9, %ymm6, %ymm6
	vcvttps2dq	%ymm6, %ymm8
	vmovups	128(%rsp), %ymm7                # 32-byte Reload
	vfmsub231ps	%ymm7, %ymm6, %ymm5     # ymm5 = (ymm6 * ymm7) - ymm5
	vmovups	480(%rsp), %ymm11               # 32-byte Reload
	vfnmsub231ps	%ymm6, %ymm11, %ymm5    # ymm5 = -(ymm11 * ymm6) - ymm5
	vmovups	928(%rsp), %ymm13               # 32-byte Reload
	vmovaps	%ymm13, %ymm6
	vfmadd213ps	96(%rsp), %ymm5, %ymm6  # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vmovups	320(%rsp), %ymm10               # 32-byte Reload
	vfmadd213ps	%ymm10, %ymm5, %ymm6    # ymm6 = (ymm5 * ymm6) + ymm10
	vfmadd213ps	288(%rsp), %ymm5, %ymm6 # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vfmadd213ps	448(%rsp), %ymm5, %ymm6 # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vfmadd213ps	416(%rsp), %ymm5, %ymm6 # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vfmadd213ps	%ymm1, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm1
	vfmadd213ps	%ymm1, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm1
	vmovdqu	352(%rsp), %ymm9                # 32-byte Reload
	vpaddd	%ymm9, %ymm8, %ymm5
	vpcmpgtd	%ymm9, %ymm8, %ymm8
	vpslld	$23, %ymm5, %ymm9
	vmulps	%ymm6, %ymm9, %ymm6
	vblendvps	%ymm8, %ymm15, %ymm6, %ymm6
	vpcmpgtd	%ymm5, %ymm3, %ymm5
	vpandn	%ymm6, %ymm5, %ymm5
	vaddps	%ymm5, %ymm4, %ymm3
	vmovups	%ymm3, 832(%rsp)                # 32-byte Spill
	vmovups	(%r14,%r15), %ymm4
	vsubps	(%rcx,%r15), %ymm4, %ymm8
	vxorps	%xmm3, %xmm3, %xmm3
	vcmpleps	%ymm3, %ymm8, %ymm4
	vblendvps	%ymm4, %ymm1, %ymm8, %ymm5
	vandps	704(%rsp), %ymm5, %ymm6         # 32-byte Folded Reload
	vorps	256(%rsp), %ymm6, %ymm6         # 32-byte Folded Reload
	vsubps	%ymm6, %ymm1, %ymm6
	vmovups	224(%rsp), %ymm9                # 32-byte Reload
	vfmadd213ps	(%rsp), %ymm6, %ymm9    # 32-byte Folded Reload
                                        # ymm9 = (ymm6 * ymm9) + mem
	vfmadd213ps	32(%rsp), %ymm6, %ymm9  # 32-byte Folded Reload
                                        # ymm9 = (ymm6 * ymm9) + mem
	vfmadd213ps	64(%rsp), %ymm6, %ymm9  # 32-byte Folded Reload
                                        # ymm9 = (ymm6 * ymm9) + mem
	vfmadd213ps	%ymm14, %ymm6, %ymm9    # ymm9 = (ymm6 * ymm9) + ymm14
	vfmadd213ps	%ymm12, %ymm6, %ymm9    # ymm9 = (ymm6 * ymm9) + ymm12
	vmovups	640(%rsp), %ymm15               # 32-byte Reload
	vfmadd213ps	%ymm15, %ymm6, %ymm9    # ymm9 = (ymm6 * ymm9) + ymm15
	vfmadd213ps	%ymm2, %ymm6, %ymm9     # ymm9 = (ymm6 * ymm9) + ymm2
	vmovups	192(%rsp), %ymm14               # 32-byte Reload
	vfmadd213ps	%ymm14, %ymm6, %ymm9    # ymm9 = (ymm6 * ymm9) + ymm14
	vfmadd213ps	%ymm1, %ymm6, %ymm9     # ymm9 = (ymm6 * ymm9) + ymm1
	vmulps	%ymm6, %ymm9, %ymm9
	movq	1320(%rsp), %rbx
	vmovups	(%rbx,%r15), %ymm6
	vsubps	(%rdx,%r15), %ymm6, %ymm6
	vpsrad	$23, %ymm5, %ymm5
	vmovdqu	736(%rsp), %ymm2                # 32-byte Reload
	vpaddd	%ymm2, %ymm5, %ymm5
	vcvtdq2ps	%ymm5, %ymm5
	vfmsub231ps	%ymm5, %ymm0, %ymm9     # ymm9 = (ymm0 * ymm5) - ymm9
	vcmpltps	%ymm3, %ymm8, %ymm5
	vmovups	512(%rsp), %ymm0                # 32-byte Reload
	vblendvps	%ymm5, 544(%rsp), %ymm0, %ymm5 # 32-byte Folded Reload
	vblendvps	%ymm4, %ymm5, %ymm9, %ymm5
	movq	1328(%rsp), %rbx
	vmovups	(%rbx,%r15), %ymm4
	vsubps	(%r8,%r15), %ymm4, %ymm12
	vmovups	%ymm12, 960(%rsp)               # 32-byte Spill
	vmulps	800(%rsp), %ymm6, %ymm3         # 32-byte Folded Reload
	vfmadd231ps	864(%rsp), %ymm8, %ymm3 # 32-byte Folded Reload
                                        # ymm3 = (ymm8 * mem) + ymm3
	vaddps	%ymm5, %ymm5, %ymm5
	vmulps	160(%rsp), %ymm5, %ymm8         # 32-byte Folded Reload
	vroundps	$9, %ymm8, %ymm8
	vcvttps2dq	%ymm8, %ymm9
	vfmsub231ps	%ymm7, %ymm8, %ymm5     # ymm5 = (ymm8 * ymm7) - ymm5
	vfnmsub231ps	%ymm8, %ymm11, %ymm5    # ymm5 = -(ymm11 * ymm8) - ymm5
	vmovaps	%ymm13, %ymm8
	vfmadd213ps	96(%rsp), %ymm5, %ymm8  # 32-byte Folded Reload
                                        # ymm8 = (ymm5 * ymm8) + mem
	vfmadd213ps	%ymm10, %ymm5, %ymm8    # ymm8 = (ymm5 * ymm8) + ymm10
	vmovups	288(%rsp), %ymm4                # 32-byte Reload
	vfmadd213ps	%ymm4, %ymm5, %ymm8     # ymm8 = (ymm5 * ymm8) + ymm4
	vmovups	448(%rsp), %ymm11               # 32-byte Reload
	vfmadd213ps	%ymm11, %ymm5, %ymm8    # ymm8 = (ymm5 * ymm8) + ymm11
	vmovups	416(%rsp), %ymm13               # 32-byte Reload
	vfmadd213ps	%ymm13, %ymm5, %ymm8    # ymm8 = (ymm5 * ymm8) + ymm13
	vfmadd213ps	%ymm1, %ymm5, %ymm8     # ymm8 = (ymm5 * ymm8) + ymm1
	vfmadd213ps	%ymm1, %ymm5, %ymm8     # ymm8 = (ymm5 * ymm8) + ymm1
	vmovdqu	352(%rsp), %ymm7                # 32-byte Reload
	vpaddd	%ymm7, %ymm9, %ymm5
	vpcmpgtd	%ymm7, %ymm9, %ymm9
	vpslld	$23, %ymm5, %ymm10
	vmulps	%ymm10, %ymm8, %ymm8
	vmovups	384(%rsp), %ymm7                # 32-byte Reload
	vblendvps	%ymm9, %ymm7, %ymm8, %ymm8
	vfmadd231ps	992(%rsp), %ymm12, %ymm3 # 32-byte Folded Reload
                                        # ymm3 = (ymm12 * mem) + ymm3
	vmovups	%ymm3, 864(%rsp)                # 32-byte Spill
	vmovdqu	896(%rsp), %ymm12               # 32-byte Reload
	vpcmpgtd	%ymm5, %ymm12, %ymm3
	vpandn	%ymm8, %ymm3, %ymm3
	vmovdqu	%ymm3, 800(%rsp)                # 32-byte Spill
	vcmpleps	__ymm@0000000000000000000000000000000000000000000000000000000000000000(%rip), %ymm6, %ymm5
	vblendvps	%ymm5, %ymm1, %ymm6, %ymm8
	vandps	704(%rsp), %ymm8, %ymm9         # 32-byte Folded Reload
	vorps	256(%rsp), %ymm9, %ymm9         # 32-byte Folded Reload
	vsubps	%ymm9, %ymm1, %ymm9
	vmovups	224(%rsp), %ymm10               # 32-byte Reload
	vfmadd213ps	(%rsp), %ymm9, %ymm10   # 32-byte Folded Reload
                                        # ymm10 = (ymm9 * ymm10) + mem
	vfmadd213ps	32(%rsp), %ymm9, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm9 * ymm10) + mem
	vfmadd213ps	64(%rsp), %ymm9, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm9 * ymm10) + mem
	vmovups	768(%rsp), %ymm3                # 32-byte Reload
	vfmadd213ps	%ymm3, %ymm9, %ymm10    # ymm10 = (ymm9 * ymm10) + ymm3
	vfmadd213ps	672(%rsp), %ymm9, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm9 * ymm10) + mem
	vfmadd213ps	%ymm15, %ymm9, %ymm10   # ymm10 = (ymm9 * ymm10) + ymm15
	vfmadd213ps	608(%rsp), %ymm9, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm9 * ymm10) + mem
	vfmadd213ps	%ymm14, %ymm9, %ymm10   # ymm10 = (ymm9 * ymm10) + ymm14
	vfmadd213ps	%ymm1, %ymm9, %ymm10    # ymm10 = (ymm9 * ymm10) + ymm1
	vmulps	%ymm10, %ymm9, %ymm9
	vcmpltps	__ymm@0000000000000000000000000000000000000000000000000000000000000000(%rip), %ymm6, %ymm6
	vpsrad	$23, %ymm8, %ymm8
	vpaddd	%ymm2, %ymm8, %ymm8
	vcvtdq2ps	%ymm8, %ymm8
	vmovups	576(%rsp), %ymm15               # 32-byte Reload
	vfmsub231ps	%ymm8, %ymm15, %ymm9    # ymm9 = (ymm15 * ymm8) - ymm9
	vmovups	544(%rsp), %ymm14               # 32-byte Reload
	vblendvps	%ymm6, %ymm14, %ymm0, %ymm6
	vblendvps	%ymm5, %ymm6, %ymm9, %ymm5
	vaddps	%ymm5, %ymm5, %ymm5
	vmulps	160(%rsp), %ymm5, %ymm6         # 32-byte Folded Reload
	vroundps	$9, %ymm6, %ymm6
	vcvttps2dq	%ymm6, %ymm8
	vfmsub231ps	128(%rsp), %ymm6, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm6 * mem) - ymm5
	vmovups	480(%rsp), %ymm0                # 32-byte Reload
	vfnmsub231ps	%ymm6, %ymm0, %ymm5     # ymm5 = -(ymm0 * ymm6) - ymm5
	vmovups	928(%rsp), %ymm10               # 32-byte Reload
	vmovaps	%ymm10, %ymm6
	vfmadd213ps	96(%rsp), %ymm5, %ymm6  # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vfmadd213ps	320(%rsp), %ymm5, %ymm6 # 32-byte Folded Reload
                                        # ymm6 = (ymm5 * ymm6) + mem
	vfmadd213ps	%ymm4, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm4
	vfmadd213ps	%ymm11, %ymm5, %ymm6    # ymm6 = (ymm5 * ymm6) + ymm11
	vfmadd213ps	%ymm13, %ymm5, %ymm6    # ymm6 = (ymm5 * ymm6) + ymm13
	vfmadd213ps	%ymm1, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm1
	vfmadd213ps	%ymm1, %ymm5, %ymm6     # ymm6 = (ymm5 * ymm6) + ymm1
	vmovdqu	352(%rsp), %ymm2                # 32-byte Reload
	vpaddd	%ymm2, %ymm8, %ymm5
	vpcmpgtd	%ymm2, %ymm8, %ymm8
	vpslld	$23, %ymm5, %ymm9
	vmulps	%ymm6, %ymm9, %ymm6
	vblendvps	%ymm8, %ymm7, %ymm6, %ymm6
	vpcmpgtd	%ymm5, %ymm12, %ymm5
	vpandn	%ymm6, %ymm5, %ymm5
	vmovups	960(%rsp), %ymm4                # 32-byte Reload
	vcmpleps	__ymm@0000000000000000000000000000000000000000000000000000000000000000(%rip), %ymm4, %ymm6
	vblendvps	%ymm6, %ymm1, %ymm4, %ymm8
	vaddps	800(%rsp), %ymm5, %ymm7         # 32-byte Folded Reload
	vandps	704(%rsp), %ymm8, %ymm5         # 32-byte Folded Reload
	vorps	256(%rsp), %ymm5, %ymm5         # 32-byte Folded Reload
	vsubps	%ymm5, %ymm1, %ymm5
	vmovups	224(%rsp), %ymm9                # 32-byte Reload
	vfmadd213ps	(%rsp), %ymm5, %ymm9    # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	32(%rsp), %ymm5, %ymm9  # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	64(%rsp), %ymm5, %ymm9  # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	%ymm3, %ymm5, %ymm9     # ymm9 = (ymm5 * ymm9) + ymm3
	vfmadd213ps	672(%rsp), %ymm5, %ymm9 # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	640(%rsp), %ymm5, %ymm9 # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	608(%rsp), %ymm5, %ymm9 # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	192(%rsp), %ymm5, %ymm9 # 32-byte Folded Reload
                                        # ymm9 = (ymm5 * ymm9) + mem
	vfmadd213ps	%ymm1, %ymm5, %ymm9     # ymm9 = (ymm5 * ymm9) + ymm1
	vmulps	%ymm5, %ymm9, %ymm5
	vxorps	%xmm9, %xmm9, %xmm9
	vcmpltps	%ymm9, %ymm4, %ymm4
	vpsrad	$23, %ymm8, %ymm8
	vpaddd	736(%rsp), %ymm8, %ymm8         # 32-byte Folded Reload
	vcvtdq2ps	%ymm8, %ymm8
	vmovups	512(%rsp), %ymm3                # 32-byte Reload
	vblendvps	%ymm4, %ymm14, %ymm3, %ymm4
	vfmsub231ps	%ymm8, %ymm15, %ymm5    # ymm5 = (ymm15 * ymm8) - ymm5
	vblendvps	%ymm6, %ymm4, %ymm5, %ymm4
	vaddps	%ymm4, %ymm4, %ymm4
	vmulps	160(%rsp), %ymm4, %ymm5         # 32-byte Folded Reload
	vroundps	$9, %ymm5, %ymm5
	vcvttps2dq	%ymm5, %ymm6
	vfmsub231ps	128(%rsp), %ymm5, %ymm4 # 32-byte Folded Reload
                                        # ymm4 = (ymm5 * mem) - ymm4
	vfnmsub231ps	%ymm5, %ymm0, %ymm4     # ymm4 = -(ymm0 * ymm5) - ymm4
	vfmadd213ps	96(%rsp), %ymm4, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm4 * ymm10) + mem
	vfmadd213ps	320(%rsp), %ymm4, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm4 * ymm10) + mem
	vfmadd213ps	288(%rsp), %ymm4, %ymm10 # 32-byte Folded Reload
                                        # ymm10 = (ymm4 * ymm10) + mem
	vfmadd213ps	%ymm11, %ymm4, %ymm10   # ymm10 = (ymm4 * ymm10) + ymm11
	vfmadd213ps	%ymm13, %ymm4, %ymm10   # ymm10 = (ymm4 * ymm10) + ymm13
	vfmadd213ps	%ymm1, %ymm4, %ymm10    # ymm10 = (ymm4 * ymm10) + ymm1
	vfmadd213ps	%ymm1, %ymm4, %ymm10    # ymm10 = (ymm4 * ymm10) + ymm1
	vpaddd	%ymm2, %ymm6, %ymm4
	vpslld	$23, %ymm4, %ymm8
	vmulps	%ymm8, %ymm10, %ymm5
	vpcmpgtd	%ymm2, %ymm6, %ymm6
	vmovups	864(%rsp), %ymm2                # 32-byte Reload
	vblendvps	%ymm6, 384(%rsp), %ymm5, %ymm5 # 32-byte Folded Reload
	vmovups	832(%rsp), %ymm6                # 32-byte Reload
	vpcmpgtd	%ymm4, %ymm12, %ymm4
	vpandn	%ymm5, %ymm4, %ymm4
	vaddps	%ymm4, %ymm7, %ymm3
	vmovups	(%r9,%r15), %ymm4
	vfnmadd213ps	%ymm3, %ymm4, %ymm4     # ymm4 = -(ymm4 * ymm4) + ymm3
	vmulps	%ymm4, %ymm6, %ymm3
	vfmsub231ps	%ymm2, %ymm2, %ymm3     # ymm3 = (ymm2 * ymm2) - ymm3
	vxorps	%xmm0, %xmm0, %xmm0
	vcmpltps	%ymm3, %ymm9, %ymm4
	vmovmskps	%ymm4, %r10d
	testb	%r10b, %r10b
	je	.LBB2_6
# %bb.8:                                # %safe_if_run_true
                                        #   in Loop: Header=BB2_5 Depth=1
	vbroadcastss	__real@80000000(%rip), %ymm5 # ymm5 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%ymm5, %ymm2, %ymm2
	vsqrtps	%ymm3, %ymm5
	vsubps	%ymm5, %ymm2, %ymm2
	vdivps	%ymm6, %ymm2, %ymm2
	vcmpltps	%ymm2, %ymm0, %ymm5
	vandps	%ymm4, %ymm5, %ymm4
	vextractf128	$1, %ymm4, %xmm5
	vpackssdw	%xmm5, %xmm4, %xmm4
	vpxor	__xmm@ffffffffffffffffffffffffffffffff(%rip), %xmm4, %xmm5
	vpacksswb	%xmm5, %xmm5, %xmm5
	vpmovmskb	%xmm5, %ebx
	cmpb	$-1, %bl
	je	.LBB2_10
# %bb.9:                                # %eval_1
                                        #   in Loop: Header=BB2_5 Depth=1
	vpmovsxwd	%xmm4, %ymm5
	vmaskmovps	(%r12,%r15), %ymm5, %ymm5
	vcmpltps	%ymm5, %ymm2, %ymm5
	vextractf128	$1, %ymm5, %xmm6
	vpackssdw	%xmm6, %xmm5, %xmm5
	vpand	%xmm4, %xmm5, %xmm4
.LBB2_10:                               # %logical_op_done
                                        #   in Loop: Header=BB2_5 Depth=1
	vpsllw	$15, %xmm4, %xmm5
	vpmovmskb	%xmm5, %ebx
	testl	$43690, %ebx                    # imm = 0xAAAA
	je	.LBB2_6
# %bb.11:                               # %safe_if_run_true127
                                        #   in Loop: Header=BB2_5 Depth=1
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpslld	$31, %ymm4, %ymm4
	vmaskmovps	%ymm2, %ymm4, (%r12,%r15)
.LBB2_6:                                # %safe_if_after_true
                                        #   in Loop: Header=BB2_5 Depth=1
	vcmpnltps	%ymm3, %ymm0, %ymm2
	vextractf128	$1, %ymm2, %xmm3
	vpackssdw	%xmm3, %xmm2, %xmm2
	vpmovmskb	%xmm2, %ebx
	testw	%bx, %bx
	je	.LBB2_7
# %bb.12:                               # %safe_if_run_false139
                                        #   in Loop: Header=BB2_5 Depth=1
	vpmovsxwd	%xmm2, %ymm2
	vbroadcastss	__real@4b189680(%rip), %ymm3 # ymm3 = [1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7]
	vmaskmovps	%ymm3, %ymm2, (%r12,%r15)
	jmp	.LBB2_7
.LBB2_1:
	xorl	%edi, %edi
.LBB2_2:                                # %partial_inner_all_outer
	movl	1368(%rsp), %eax
	cmpl	%eax, %edi
	jge	.LBB2_3
# %bb.13:                               # %partial_inner_only
	vmovd	%edi, %xmm0
	vpbroadcastd	%xmm0, %ymm0
	vpor	__ymm@0000000700000006000000050000000400000003000000020000000100000000(%rip), %ymm0, %ymm0
	vmovd	%eax, %xmm1
	vpbroadcastd	%xmm1, %ymm1
	vpcmpgtd	%ymm0, %ymm1, %ymm9
	shll	$2, %edi
	movslq	%edi, %rbp
	vmaskmovps	(%rsi,%rbp), %ymm9, %ymm1
	vpxor	%xmm0, %xmm0, %xmm0
	vcmpleps	%ymm0, %ymm1, %ymm3
	vxorps	%xmm11, %xmm11, %xmm11
	vbroadcastss	__real@3f800000(%rip), %ymm4 # ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
	vblendvps	%ymm3, %ymm4, %ymm1, %ymm0
	vmovaps	%ymm1, %ymm8
	vmovups	%ymm1, 736(%rsp)                # 32-byte Spill
	vbroadcastss	__real@807fffff(%rip), %ymm14 # ymm14 = [2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255,2155872255]
	vandps	%ymm0, %ymm14, %ymm1
	vbroadcastss	__real@3f000000(%rip), %ymm13 # ymm13 = [1056964608,1056964608,1056964608,1056964608,1056964608,1056964608,1056964608,1056964608]
	vorps	%ymm1, %ymm13, %ymm1
	vmovups	%ymm13, 352(%rsp)               # 32-byte Spill
	vsubps	%ymm1, %ymm4, %ymm1
	vbroadcastss	__real@7fc00000(%rip), %ymm2 # ymm2 = [NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN]
	vblendvps	%ymm9, %ymm1, %ymm2, %ymm6
	vbroadcastss	__real@3fdfe28e(%rip), %ymm5 # ymm5 = [1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0,1.7491014E+0]
	vbroadcastss	__real@c01f5af7(%rip), %ymm1 # ymm1 = [-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0,-2.48992705E+0]
	vmovaps	%ymm5, %ymm7
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vmovaps	%ymm1, %ymm15
	vbroadcastss	__real@3ffe0193(%rip), %ymm12 # ymm12 = [1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0,1.98442304E+0]
	vfmadd213ps	%ymm12, %ymm6, %ymm7    # ymm7 = (ymm6 * ymm7) + ymm12
	vmovups	%ymm12, 192(%rsp)               # 32-byte Spill
	vbroadcastss	__real@bf198181(%rip), %ymm1 # ymm1 = [-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1,-5.99632323E-1]
	vmovups	%ymm1, 160(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vbroadcastss	__real@3eaf548d(%rip), %ymm1 # ymm1 = [3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1,3.42441946E-1]
	vmovups	%ymm1, 128(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vbroadcastss	__real@3e33a0af(%rip), %ymm1 # ymm1 = [1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1,1.75417647E-1]
	vmovups	%ymm1, 96(%rsp)                 # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vbroadcastss	__real@3e80fb87(%rip), %ymm1 # ymm1 = [2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1,2.51919001E-1]
	vmovups	%ymm1, 32(%rsp)                 # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vbroadcastss	__real@3eaaa11c(%rip), %ymm1 # ymm1 = [3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1,3.33260417E-1]
	vmovups	%ymm1, (%rsp)                   # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vbroadcastss	__real@3f000010(%rip), %ymm1 # ymm1 = [5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1,5.00000954E-1]
	vmovups	%ymm1, 64(%rsp)                 # 32-byte Spill
	vfmadd213ps	%ymm1, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm1
	vfmadd213ps	%ymm4, %ymm6, %ymm7     # ymm7 = (ymm6 * ymm7) + ymm4
	vmulps	%ymm7, %ymm6, %ymm7
	vpsrad	$23, %ymm0, %ymm0
	vpbroadcastd	__real@ffffff82(%rip), %ymm1 # ymm1 = [4294967170,4294967170,4294967170,4294967170,4294967170,4294967170,4294967170,4294967170]
	vmovdqu	%ymm1, 256(%rsp)                # 32-byte Spill
	vpaddd	%ymm1, %ymm0, %ymm0
	vcvtdq2ps	%ymm0, %ymm0
	vbroadcastss	__real@3f317218(%rip), %ymm1 # ymm1 = [6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1,6.93147182E-1]
	vfmsub231ps	%ymm0, %ymm1, %ymm7     # ymm7 = (ymm1 * ymm0) - ymm7
	vmovups	%ymm1, 320(%rsp)                # 32-byte Spill
	vcmpltps	%ymm11, %ymm8, %ymm8
	vbroadcastss	__real@ff800000(%rip), %ymm0 # ymm0 = [-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf]
	vblendvps	%ymm8, %ymm2, %ymm0, %ymm8
	vmovaps	%ymm0, %ymm6
	vmovups	%ymm0, 224(%rsp)                # 32-byte Spill
	vblendvps	%ymm3, %ymm8, %ymm7, %ymm0
	vmovups	%ymm0, 704(%rsp)                # 32-byte Spill
	vmaskmovps	(%r11,%rbp), %ymm9, %ymm0
	vcmpleps	%ymm11, %ymm0, %ymm3
	vblendvps	%ymm3, %ymm4, %ymm0, %ymm7
	vmovaps	%ymm0, %ymm11
	vmovups	%ymm0, 672(%rsp)                # 32-byte Spill
	vmovaps	%ymm14, %ymm0
	vmovups	%ymm14, 288(%rsp)               # 32-byte Spill
	vandps	%ymm7, %ymm14, %ymm8
	vorps	%ymm13, %ymm8, %ymm8
	vsubps	%ymm8, %ymm4, %ymm8
	vblendvps	%ymm9, %ymm8, %ymm2, %ymm8
	vmovaps	%ymm5, %ymm13
	vmovaps	%ymm15, %ymm10
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vfmadd213ps	%ymm12, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm12
	vmovups	160(%rsp), %ymm12               # 32-byte Reload
	vfmadd213ps	%ymm12, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm12
	vmovups	128(%rsp), %ymm14               # 32-byte Reload
	vfmadd213ps	%ymm14, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm14
	vmovups	96(%rsp), %ymm15                # 32-byte Reload
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vfmadd213ps	32(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	(%rsp), %ymm8, %ymm13   # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	64(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	%ymm4, %ymm8, %ymm13    # ymm13 = (ymm8 * ymm13) + ymm4
	vmulps	%ymm13, %ymm8, %ymm8
	vpsrad	$23, %ymm7, %ymm7
	vpaddd	256(%rsp), %ymm7, %ymm7         # 32-byte Folded Reload
	vcvtdq2ps	%ymm7, %ymm7
	vfmsub231ps	%ymm7, %ymm1, %ymm8     # ymm8 = (ymm1 * ymm7) - ymm8
	vxorps	%xmm13, %xmm13, %xmm13
	vcmpltps	%ymm13, %ymm11, %ymm7
	vblendvps	%ymm7, %ymm2, %ymm6, %ymm7
	vblendvps	%ymm3, %ymm7, %ymm8, %ymm1
	vmovups	%ymm1, 640(%rsp)                # 32-byte Spill
	vmaskmovps	(%r13,%rbp), %ymm9, %ymm1
	vcmpleps	%ymm13, %ymm1, %ymm3
	vblendvps	%ymm3, %ymm4, %ymm1, %ymm7
	vmovups	%ymm1, 608(%rsp)                # 32-byte Spill
	vandps	%ymm0, %ymm7, %ymm8
	vmovups	352(%rsp), %ymm6                # 32-byte Reload
	vorps	%ymm6, %ymm8, %ymm8
	vsubps	%ymm8, %ymm4, %ymm8
	vblendvps	%ymm9, %ymm8, %ymm2, %ymm8
	vmovaps	%ymm5, %ymm13
	vfmadd213ps	%ymm10, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm10
	vmovaps	%ymm10, %ymm0
	vmovups	192(%rsp), %ymm10               # 32-byte Reload
	vfmadd213ps	%ymm10, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm10
	vmovaps	%ymm12, %ymm11
	vfmadd213ps	%ymm12, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm12
	vmovaps	%ymm14, %ymm12
	vfmadd213ps	%ymm14, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm14
	vmovaps	%ymm15, %ymm14
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vmovups	32(%rsp), %ymm15                # 32-byte Reload
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vfmadd213ps	(%rsp), %ymm8, %ymm13   # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	64(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	%ymm4, %ymm8, %ymm13    # ymm13 = (ymm8 * ymm13) + ymm4
	vmulps	%ymm13, %ymm8, %ymm8
	vpsrad	$23, %ymm7, %ymm7
	vpaddd	256(%rsp), %ymm7, %ymm7         # 32-byte Folded Reload
	vcvtdq2ps	%ymm7, %ymm7
	vfmsub231ps	320(%rsp), %ymm7, %ymm8 # 32-byte Folded Reload
                                        # ymm8 = (ymm7 * mem) - ymm8
	vxorps	%xmm13, %xmm13, %xmm13
	vcmpltps	%ymm13, %ymm1, %ymm7
	vmovups	224(%rsp), %ymm1                # 32-byte Reload
	vblendvps	%ymm7, %ymm2, %ymm1, %ymm7
	vblendvps	%ymm3, %ymm7, %ymm8, %ymm1
	vmovups	%ymm1, 576(%rsp)                # 32-byte Spill
	vmaskmovps	(%r14,%rbp), %ymm9, %ymm3
	vmaskmovps	(%rcx,%rbp), %ymm9, %ymm7
	vsubps	%ymm7, %ymm3, %ymm1
	vcmpleps	%ymm13, %ymm1, %ymm3
	vblendvps	%ymm3, %ymm4, %ymm1, %ymm7
	vmovups	%ymm1, 544(%rsp)                # 32-byte Spill
	vandps	288(%rsp), %ymm7, %ymm8         # 32-byte Folded Reload
	vorps	%ymm6, %ymm8, %ymm8
	vsubps	%ymm8, %ymm4, %ymm8
	vblendvps	%ymm9, %ymm8, %ymm2, %ymm8
	vmovaps	%ymm5, %ymm13
	vmovups	%ymm0, 768(%rsp)                # 32-byte Spill
	vfmadd213ps	%ymm0, %ymm8, %ymm13    # ymm13 = (ymm8 * ymm13) + ymm0
	vfmadd213ps	%ymm10, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm10
	vfmadd213ps	%ymm11, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm11
	vfmadd213ps	%ymm12, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm12
	vfmadd213ps	%ymm14, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm14
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vfmadd213ps	(%rsp), %ymm8, %ymm13   # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vmovups	64(%rsp), %ymm10                # 32-byte Reload
	vfmadd213ps	%ymm10, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm10
	vfmadd213ps	%ymm4, %ymm8, %ymm13    # ymm13 = (ymm8 * ymm13) + ymm4
	vmulps	%ymm13, %ymm8, %ymm8
	vpsrad	$23, %ymm7, %ymm7
	vmovdqu	256(%rsp), %ymm6                # 32-byte Reload
	vpaddd	%ymm6, %ymm7, %ymm7
	vcvtdq2ps	%ymm7, %ymm7
	vmovups	320(%rsp), %ymm11               # 32-byte Reload
	vfmsub231ps	%ymm7, %ymm11, %ymm8    # ymm8 = (ymm11 * ymm7) - ymm8
	vcmpltps	__ymm@0000000000000000000000000000000000000000000000000000000000000000(%rip), %ymm1, %ymm7
	vmovups	224(%rsp), %ymm0                # 32-byte Reload
	vblendvps	%ymm7, %ymm2, %ymm0, %ymm7
	vmovaps	%ymm2, %ymm13
	vblendvps	%ymm3, %ymm7, %ymm8, %ymm1
	vmovups	%ymm1, 512(%rsp)                # 32-byte Spill
	movq	1320(%rsp), %rax
	vmaskmovps	(%rax,%rbp), %ymm9, %ymm3
	vmaskmovps	(%rdx,%rbp), %ymm9, %ymm7
	vmovaps	%ymm9, %ymm2
	vsubps	%ymm7, %ymm3, %ymm1
	vxorps	%xmm3, %xmm3, %xmm3
	vcmpleps	%ymm3, %ymm1, %ymm3
	vblendvps	%ymm3, %ymm4, %ymm1, %ymm7
	vmovups	%ymm1, 480(%rsp)                # 32-byte Spill
	vmovups	288(%rsp), %ymm12               # 32-byte Reload
	vandps	%ymm7, %ymm12, %ymm8
	vmovups	352(%rsp), %ymm14               # 32-byte Reload
	vorps	%ymm14, %ymm8, %ymm8
	vsubps	%ymm8, %ymm4, %ymm8
	vmovaps	%ymm13, %ymm9
	vblendvps	%ymm2, %ymm8, %ymm13, %ymm8
	vmovaps	%ymm5, %ymm13
	vmovups	768(%rsp), %ymm15               # 32-byte Reload
	vfmadd213ps	%ymm15, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm15
	vfmadd213ps	192(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	160(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	128(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	96(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	32(%rsp), %ymm8, %ymm13 # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	(%rsp), %ymm8, %ymm13   # 32-byte Folded Reload
                                        # ymm13 = (ymm8 * ymm13) + mem
	vfmadd213ps	%ymm10, %ymm8, %ymm13   # ymm13 = (ymm8 * ymm13) + ymm10
	vfmadd213ps	%ymm4, %ymm8, %ymm13    # ymm13 = (ymm8 * ymm13) + ymm4
	vmulps	%ymm13, %ymm8, %ymm8
	vpsrad	$23, %ymm7, %ymm7
	vpaddd	%ymm6, %ymm7, %ymm7
	vcmpltps	__ymm@0000000000000000000000000000000000000000000000000000000000000000(%rip), %ymm1, %ymm13
	vblendvps	%ymm13, %ymm9, %ymm0, %ymm13
	vcvtdq2ps	%ymm7, %ymm7
	movq	1328(%rsp), %rax
	vmaskmovps	(%rax,%rbp), %ymm2, %ymm0
	vfmsub231ps	%ymm7, %ymm11, %ymm8    # ymm8 = (ymm11 * ymm7) - ymm8
	vmaskmovps	(%r8,%rbp), %ymm2, %ymm7
	vmovups	%ymm2, 416(%rsp)                # 32-byte Spill
	vblendvps	%ymm3, %ymm13, %ymm8, %ymm1
	vmovups	%ymm1, 448(%rsp)                # 32-byte Spill
	vsubps	%ymm7, %ymm0, %ymm1
	vxorps	%xmm8, %xmm8, %xmm8
	vcmpleps	%ymm8, %ymm1, %ymm0
	vmovaps	%ymm4, %ymm10
	vblendvps	%ymm0, %ymm4, %ymm1, %ymm3
	vmovups	%ymm1, 384(%rsp)                # 32-byte Spill
	vandps	%ymm3, %ymm12, %ymm7
	vorps	%ymm7, %ymm14, %ymm7
	vpsrad	$23, %ymm3, %ymm3
	vsubps	%ymm7, %ymm4, %ymm7
	vblendvps	%ymm2, %ymm7, %ymm9, %ymm7
	vpaddd	%ymm6, %ymm3, %ymm3
	vfmadd213ps	%ymm15, %ymm7, %ymm5    # ymm5 = (ymm7 * ymm5) + ymm15
	vfmadd213ps	192(%rsp), %ymm7, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	160(%rsp), %ymm7, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	128(%rsp), %ymm7, %ymm5 # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	96(%rsp), %ymm7, %ymm5  # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	32(%rsp), %ymm7, %ymm5  # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	(%rsp), %ymm7, %ymm5    # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	64(%rsp), %ymm7, %ymm5  # 32-byte Folded Reload
                                        # ymm5 = (ymm7 * ymm5) + mem
	vfmadd213ps	%ymm4, %ymm7, %ymm5     # ymm5 = (ymm7 * ymm5) + ymm4
	vmulps	%ymm5, %ymm7, %ymm5
	vcvtdq2ps	%ymm3, %ymm3
	vfmsub231ps	%ymm3, %ymm11, %ymm5    # ymm5 = (ymm11 * ymm3) - ymm5
	vcmpltps	%ymm8, %ymm1, %ymm3
	vmovups	224(%rsp), %ymm1                # 32-byte Reload
	vblendvps	%ymm3, %ymm9, %ymm1, %ymm1
	vblendvps	%ymm0, %ymm1, %ymm5, %ymm0
	vmovups	%ymm0, 224(%rsp)                # 32-byte Spill
	vmovups	704(%rsp), %ymm0                # 32-byte Reload
	vaddps	%ymm0, %ymm0, %ymm3
	vbroadcastss	__real@3fb8aa3b(%rip), %ymm11 # ymm11 = [1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0,1.44269502E+0]
	vmulps	%ymm3, %ymm11, %ymm0
	vroundps	$9, %ymm0, %ymm6
	vbroadcastss	__real@3f317200(%rip), %ymm5 # ymm5 = [6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1,6.93145751E-1]
	vfmsub231ps	%ymm5, %ymm6, %ymm3     # ymm3 = (ymm6 * ymm5) - ymm3
	vbroadcastss	__real@35bfbe8e(%rip), %ymm13 # ymm13 = [1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6,1.42860677E-6]
	vfnmsub231ps	%ymm6, %ymm13, %ymm3    # ymm3 = -(ymm13 * ymm6) - ymm3
	vmovups	%ymm13, 32(%rsp)                # 32-byte Spill
	vbroadcastss	__real@39907835(%rip), %ymm1 # ymm1 = [2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4,2.75553815E-4]
	vbroadcastss	__real@3aaaf7b5(%rip), %ymm4 # ymm4 = [1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3,1.30437932E-3]
	vmovaps	%ymm1, %ymm7
	vfmadd213ps	%ymm4, %ymm3, %ymm7     # ymm7 = (ymm3 * ymm7) + ymm4
	vmovups	%ymm4, (%rsp)                   # 32-byte Spill
	vbroadcastss	__real@3c09475d(%rip), %ymm14 # ymm14 = [8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3,8.37883074E-3]
	vfmadd213ps	%ymm14, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm14
	vbroadcastss	__real@3d2a9d49(%rip), %ymm15 # ymm15 = [4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2,4.16539051E-2]
	vfmadd213ps	%ymm15, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm15
	vbroadcastss	__real@3e2aab20(%rip), %ymm12 # ymm12 = [1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1,1.66668415E-1]
	vfmadd213ps	%ymm12, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm12
	vbroadcastss	__real@3efffffd(%rip), %ymm9 # ymm9 = [4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1,4.99999911E-1]
	vfmadd213ps	%ymm9, %ymm3, %ymm7     # ymm7 = (ymm3 * ymm7) + ymm9
	vmovaps	%ymm10, %ymm0
	vfmadd213ps	%ymm10, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm10
	vfmadd213ps	%ymm10, %ymm3, %ymm7    # ymm7 = (ymm3 * ymm7) + ymm10
	vcvttps2dq	%ymm6, %ymm3
	vpbroadcastd	__real@0000007f(%rip), %ymm10 # ymm10 = [127,127,127,127,127,127,127,127]
	vpaddd	%ymm3, %ymm10, %ymm2
	vmovdqu	%ymm2, 64(%rsp)                 # 32-byte Spill
	vpslld	$23, %ymm2, %ymm8
	vmulps	%ymm7, %ymm8, %ymm8
	vpcmpgtd	%ymm10, %ymm3, %ymm3
	vbroadcastss	__real@7f800000(%rip), %ymm7 # ymm7 = [+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf]
	vblendvps	%ymm3, %ymm7, %ymm8, %ymm2
	vmovups	%ymm2, 256(%rsp)                # 32-byte Spill
	vmovups	640(%rsp), %ymm2                # 32-byte Reload
	vaddps	%ymm2, %ymm2, %ymm3
	vmulps	%ymm3, %ymm11, %ymm8
	vroundps	$9, %ymm8, %ymm8
	vfmsub231ps	%ymm5, %ymm8, %ymm3     # ymm3 = (ymm8 * ymm5) - ymm3
	vfnmsub231ps	%ymm8, %ymm13, %ymm3    # ymm3 = -(ymm13 * ymm8) - ymm3
	vmovaps	%ymm1, %ymm13
	vfmadd213ps	%ymm4, %ymm3, %ymm13    # ymm13 = (ymm3 * ymm13) + ymm4
	vfmadd213ps	%ymm14, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm13) + ymm14
	vfmadd213ps	%ymm15, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm13) + ymm15
	vfmadd213ps	%ymm12, %ymm3, %ymm13   # ymm13 = (ymm3 * ymm13) + ymm12
	vfmadd213ps	%ymm9, %ymm3, %ymm13    # ymm13 = (ymm3 * ymm13) + ymm9
	vfmadd213ps	%ymm0, %ymm3, %ymm13    # ymm13 = (ymm3 * ymm13) + ymm0
	vfmadd213ps	%ymm0, %ymm3, %ymm13    # ymm13 = (ymm3 * ymm13) + ymm0
	vcvttps2dq	%ymm8, %ymm8
	vpaddd	%ymm10, %ymm8, %ymm2
	vmovdqu	%ymm2, 192(%rsp)                # 32-byte Spill
	vpslld	$23, %ymm2, %ymm6
	vmulps	%ymm6, %ymm13, %ymm6
	vpcmpgtd	%ymm10, %ymm8, %ymm8
	vblendvps	%ymm8, %ymm7, %ymm6, %ymm2
	vmovups	%ymm2, 160(%rsp)                # 32-byte Spill
	vmovups	576(%rsp), %ymm2                # 32-byte Reload
	vaddps	%ymm2, %ymm2, %ymm6
	vmulps	%ymm6, %ymm11, %ymm8
	vroundps	$9, %ymm8, %ymm8
	vfmsub231ps	%ymm5, %ymm8, %ymm6     # ymm6 = (ymm8 * ymm5) - ymm6
	vmovups	32(%rsp), %ymm3                 # 32-byte Reload
	vfnmsub231ps	%ymm8, %ymm3, %ymm6     # ymm6 = -(ymm3 * ymm8) - ymm6
	vmovaps	%ymm1, %ymm4
	vmovups	(%rsp), %ymm13                  # 32-byte Reload
	vfmadd213ps	%ymm13, %ymm6, %ymm4    # ymm4 = (ymm6 * ymm4) + ymm13
	vfmadd213ps	%ymm14, %ymm6, %ymm4    # ymm4 = (ymm6 * ymm4) + ymm14
	vfmadd213ps	%ymm15, %ymm6, %ymm4    # ymm4 = (ymm6 * ymm4) + ymm15
	vfmadd213ps	%ymm12, %ymm6, %ymm4    # ymm4 = (ymm6 * ymm4) + ymm12
	vfmadd213ps	%ymm9, %ymm6, %ymm4     # ymm4 = (ymm6 * ymm4) + ymm9
	vfmadd213ps	%ymm0, %ymm6, %ymm4     # ymm4 = (ymm6 * ymm4) + ymm0
	vfmadd213ps	%ymm0, %ymm6, %ymm4     # ymm4 = (ymm6 * ymm4) + ymm0
	vcvttps2dq	%ymm8, %ymm6
	vpaddd	%ymm6, %ymm10, %ymm2
	vmovdqu	%ymm2, 128(%rsp)                # 32-byte Spill
	vpslld	$23, %ymm2, %ymm8
	vmulps	%ymm4, %ymm8, %ymm4
	vpcmpgtd	%ymm10, %ymm6, %ymm6
	vblendvps	%ymm6, %ymm7, %ymm4, %ymm2
	vmovups	%ymm2, 96(%rsp)                 # 32-byte Spill
	vmovups	512(%rsp), %ymm2                # 32-byte Reload
	vaddps	%ymm2, %ymm2, %ymm4
	vmulps	%ymm4, %ymm11, %ymm6
	vroundps	$9, %ymm6, %ymm6
	vfmsub231ps	%ymm5, %ymm6, %ymm4     # ymm4 = (ymm6 * ymm5) - ymm4
	vfnmsub231ps	%ymm6, %ymm3, %ymm4     # ymm4 = -(ymm3 * ymm6) - ymm4
	vmovaps	%ymm1, %ymm8
	vfmadd213ps	%ymm13, %ymm4, %ymm8    # ymm8 = (ymm4 * ymm8) + ymm13
	vfmadd213ps	%ymm14, %ymm4, %ymm8    # ymm8 = (ymm4 * ymm8) + ymm14
	vfmadd213ps	%ymm15, %ymm4, %ymm8    # ymm8 = (ymm4 * ymm8) + ymm15
	vfmadd213ps	%ymm12, %ymm4, %ymm8    # ymm8 = (ymm4 * ymm8) + ymm12
	vfmadd213ps	%ymm9, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm9
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vfmadd213ps	%ymm0, %ymm4, %ymm8     # ymm8 = (ymm4 * ymm8) + ymm0
	vmovaps	%ymm0, %ymm2
	vcvttps2dq	%ymm6, %ymm4
	vpaddd	%ymm4, %ymm10, %ymm0
	vmovdqu	%ymm0, 288(%rsp)                # 32-byte Spill
	vpslld	$23, %ymm0, %ymm3
	vmulps	%ymm3, %ymm8, %ymm3
	vpcmpgtd	%ymm10, %ymm4, %ymm4
	vblendvps	%ymm4, %ymm7, %ymm3, %ymm0
	vmovups	%ymm0, 320(%rsp)                # 32-byte Spill
	vmovups	480(%rsp), %ymm0                # 32-byte Reload
	vmulps	672(%rsp), %ymm0, %ymm8         # 32-byte Folded Reload
	vmovups	448(%rsp), %ymm0                # 32-byte Reload
	vaddps	%ymm0, %ymm0, %ymm4
	vmulps	%ymm4, %ymm11, %ymm3
	vroundps	$9, %ymm3, %ymm3
	vfmsub231ps	%ymm5, %ymm3, %ymm4     # ymm4 = (ymm3 * ymm5) - ymm4
	vmovups	32(%rsp), %ymm6                 # 32-byte Reload
	vfnmsub231ps	%ymm3, %ymm6, %ymm4     # ymm4 = -(ymm6 * ymm3) - ymm4
	vmovaps	%ymm1, %ymm0
	vfmadd213ps	%ymm13, %ymm4, %ymm0    # ymm0 = (ymm4 * ymm0) + ymm13
	vfmadd213ps	%ymm14, %ymm4, %ymm0    # ymm0 = (ymm4 * ymm0) + ymm14
	vfmadd213ps	%ymm15, %ymm4, %ymm0    # ymm0 = (ymm4 * ymm0) + ymm15
	vfmadd213ps	%ymm12, %ymm4, %ymm0    # ymm0 = (ymm4 * ymm0) + ymm12
	vfmadd213ps	%ymm9, %ymm4, %ymm0     # ymm0 = (ymm4 * ymm0) + ymm9
	vfmadd213ps	%ymm2, %ymm4, %ymm0     # ymm0 = (ymm4 * ymm0) + ymm2
	vfmadd213ps	%ymm2, %ymm4, %ymm0     # ymm0 = (ymm4 * ymm0) + ymm2
	vcvttps2dq	%ymm3, %ymm3
	vpaddd	%ymm3, %ymm10, %ymm4
	vpslld	$23, %ymm4, %ymm13
	vmulps	%ymm0, %ymm13, %ymm0
	vpcmpgtd	%ymm10, %ymm3, %ymm3
	vblendvps	%ymm3, %ymm7, %ymm0, %ymm0
	vmovups	544(%rsp), %ymm3                # 32-byte Reload
	vfmadd231ps	736(%rsp), %ymm3, %ymm8 # 32-byte Folded Reload
                                        # ymm8 = (ymm3 * mem) + ymm8
	vmovups	224(%rsp), %ymm3                # 32-byte Reload
	vaddps	%ymm3, %ymm3, %ymm3
	vmulps	%ymm3, %ymm11, %ymm11
	vroundps	$9, %ymm11, %ymm11
	vfmsub213ps	%ymm3, %ymm11, %ymm5    # ymm5 = (ymm11 * ymm5) - ymm3
	vfnmsub231ps	%ymm6, %ymm11, %ymm5    # ymm5 = -(ymm11 * ymm6) - ymm5
	vfmadd213ps	(%rsp), %ymm5, %ymm1    # 32-byte Folded Reload
                                        # ymm1 = (ymm5 * ymm1) + mem
	vfmadd213ps	%ymm14, %ymm5, %ymm1    # ymm1 = (ymm5 * ymm1) + ymm14
	vfmadd213ps	%ymm15, %ymm5, %ymm1    # ymm1 = (ymm5 * ymm1) + ymm15
	vfmadd213ps	%ymm12, %ymm5, %ymm1    # ymm1 = (ymm5 * ymm1) + ymm12
	vfmadd213ps	%ymm9, %ymm5, %ymm1     # ymm1 = (ymm5 * ymm1) + ymm9
	vfmadd213ps	%ymm2, %ymm5, %ymm1     # ymm1 = (ymm5 * ymm1) + ymm2
	vfmadd213ps	%ymm2, %ymm5, %ymm1     # ymm1 = (ymm5 * ymm1) + ymm2
	vcvttps2dq	%ymm11, %ymm2
	vpaddd	%ymm2, %ymm10, %ymm3
	vpcmpgtd	%ymm10, %ymm2, %ymm2
	vpslld	$23, %ymm3, %ymm5
	vmulps	%ymm5, %ymm1, %ymm1
	vblendvps	%ymm2, %ymm7, %ymm1, %ymm2
	vmovups	384(%rsp), %ymm1                # 32-byte Reload
	vfmadd231ps	608(%rsp), %ymm1, %ymm8 # 32-byte Folded Reload
                                        # ymm8 = (ymm1 * mem) + ymm8
	vpbroadcastd	__real@00000001(%rip), %ymm5 # ymm5 = [1,1,1,1,1,1,1,1]
	vpcmpgtd	64(%rsp), %ymm5, %ymm1          # 32-byte Folded Reload
	vpandn	256(%rsp), %ymm1, %ymm1         # 32-byte Folded Reload
	vpcmpgtd	192(%rsp), %ymm5, %ymm7         # 32-byte Folded Reload
	vpandn	160(%rsp), %ymm7, %ymm7         # 32-byte Folded Reload
	vaddps	%ymm7, %ymm1, %ymm1
	vpcmpgtd	128(%rsp), %ymm5, %ymm7         # 32-byte Folded Reload
	vpandn	96(%rsp), %ymm7, %ymm7          # 32-byte Folded Reload
	vaddps	%ymm7, %ymm1, %ymm1
	vpcmpgtd	288(%rsp), %ymm5, %ymm6         # 32-byte Folded Reload
	vpandn	320(%rsp), %ymm6, %ymm6         # 32-byte Folded Reload
	vpcmpgtd	%ymm4, %ymm5, %ymm4
	vpandn	%ymm0, %ymm4, %ymm0
	vaddps	%ymm0, %ymm6, %ymm0
	vxorps	%xmm6, %xmm6, %xmm6
	vpcmpgtd	%ymm3, %ymm5, %ymm3
	vmovups	416(%rsp), %ymm5                # 32-byte Reload
	vpandn	%ymm2, %ymm3, %ymm2
	vmaskmovps	(%r9,%rbp), %ymm5, %ymm3
	vaddps	%ymm2, %ymm0, %ymm0
	vfnmadd213ps	%ymm0, %ymm3, %ymm3     # ymm3 = -(ymm3 * ymm3) + ymm0
	vmulps	%ymm3, %ymm1, %ymm0
	vfmsub231ps	%ymm8, %ymm8, %ymm0     # ymm0 = (ymm8 * ymm8) - ymm0
	vcmpltps	%ymm0, %ymm6, %ymm2
	vandps	%ymm5, %ymm2, %ymm2
	vmovmskps	%ymm2, %eax
	testb	%al, %al
	je	.LBB2_14
# %bb.16:                               # %safe_if_run_true281
	vextractf128	$1, %ymm2, %xmm3
	vpackssdw	%xmm3, %xmm2, %xmm2
	vbroadcastss	__real@80000000(%rip), %ymm3 # ymm3 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
	vxorps	%ymm3, %ymm8, %ymm3
	vsqrtps	%ymm0, %ymm4
	vsubps	%ymm4, %ymm3, %ymm3
	vdivps	%ymm1, %ymm3, %ymm1
	vcmpltps	%ymm1, %ymm6, %ymm3
	vextractf128	$1, %ymm3, %xmm4
	vpackssdw	%xmm4, %xmm3, %xmm3
	vpand	%xmm2, %xmm3, %xmm2
	vpcmpeqd	%xmm3, %xmm3, %xmm3
	vpxor	%xmm3, %xmm2, %xmm3
	vpacksswb	%xmm3, %xmm3, %xmm3
	vpmovmskb	%xmm3, %eax
	cmpb	$-1, %al
	je	.LBB2_18
# %bb.17:                               # %eval_1292
	vpmovsxwd	%xmm2, %ymm3
	vmaskmovps	(%r12,%rbp), %ymm3, %ymm3
	vcmpltps	%ymm3, %ymm1, %ymm3
	vextractf128	$1, %ymm3, %xmm4
	vpackssdw	%xmm4, %xmm3, %xmm3
	vpand	%xmm2, %xmm3, %xmm2
.LBB2_18:                               # %logical_op_done293
	vpsllw	$15, %xmm2, %xmm3
	vpmovmskb	%xmm3, %eax
	testl	$43690, %eax                    # imm = 0xAAAA
	je	.LBB2_14
# %bb.19:                               # %safe_if_run_true314
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpslld	$31, %ymm2, %ymm2
	vmaskmovps	%ymm1, %ymm2, (%r12,%rbp)
.LBB2_14:                               # %safe_if_after_true280
	vcmpnltps	%ymm0, %ymm6, %ymm0
	vandps	%ymm0, %ymm5, %ymm0
	vextractf128	$1, %ymm0, %xmm1
	vpackssdw	%xmm1, %xmm0, %xmm0
	vpmovmskb	%xmm0, %eax
	testw	%ax, %ax
	je	.LBB2_3
# %bb.15:                               # %safe_if_run_false334
	vpmovsxwd	%xmm0, %ymm0
	vbroadcastss	__real@4b189680(%rip), %ymm1 # ymm1 = [1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7,1.0E+7]
	vmaskmovps	%ymm1, %ymm0, (%r12,%rbp)
.LBB2_3:                                # %foreach_reset
	vmovaps	1040(%rsp), %xmm6               # 16-byte Reload
	vmovaps	1056(%rsp), %xmm7               # 16-byte Reload
	vmovaps	1072(%rsp), %xmm8               # 16-byte Reload
	vmovaps	1088(%rsp), %xmm9               # 16-byte Reload
	vmovaps	1104(%rsp), %xmm10              # 16-byte Reload
	vmovaps	1120(%rsp), %xmm11              # 16-byte Reload
	vmovaps	1136(%rsp), %xmm12              # 16-byte Reload
	vmovaps	1152(%rsp), %xmm13              # 16-byte Reload
	vmovaps	1168(%rsp), %xmm14              # 16-byte Reload
	vmovaps	1184(%rsp), %xmm15              # 16-byte Reload
	addq	$1208, %rsp                     # imm = 0x4B8
	popq	%rbx
	popq	%rbp
	popq	%rdi
	popq	%rsi
	popq	%r12
	popq	%r13
	popq	%r14
	popq	%r15
	vzeroupper
	retq
                                        # -- End function
	.section	.drectve,"yn"
	.ascii	" /FAILIFMISMATCH:\"_CRT_STDIO_ISO_WIDE_SPECIFIERS=0\""
	.globl	_fltused
